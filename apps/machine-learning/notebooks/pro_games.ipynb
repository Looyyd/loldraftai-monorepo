{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from functools import lru_cache\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE_URL = \"https://gol.gg\"\n",
    "TOURNAMENT_URL = \"https://gol.gg/tournament/tournament-matchlist/LEC%20Winter%202025/\"\n",
    "\n",
    "@lru_cache(maxsize=200)\n",
    "def get_champion_name(champion_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the champion page and parse out its <h1> text.\n",
    "    \"\"\"\n",
    "    print(f\"[DEBUG] Fetching champion page: {champion_url}\")\n",
    "    time.sleep(0.5)  # be polite\n",
    "    resp = requests.get(champion_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    h1 = soup.find(\"h1\")\n",
    "    if not h1:\n",
    "        print(\"[DEBUG] Could not find <h1> on champion page.\")\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    champion_name = h1.get_text(strip=True)\n",
    "    print(f\"[DEBUG] Champion name found: {champion_name}\")\n",
    "    return champion_name\n",
    "\n",
    "def get_picked_champions(side_div) -> list[str]:\n",
    "    \"\"\"\n",
    "    Find the row where <div class=\"col-2\"> contains the text \"Picks\".\n",
    "    Then fetch all <a> elements in the sibling <div class=\"col-10\">,\n",
    "    and parse champion names from those links.\n",
    "    \"\"\"\n",
    "    picks_row = None\n",
    "    \n",
    "    # Find the row that actually contains the text \"Picks\" in a col-2\n",
    "    for row in side_div.find_all(\"div\", class_=\"row\"):\n",
    "        label_div = row.find(\"div\", class_=\"col-2\")\n",
    "        if label_div and label_div.get_text(strip=True) == \"Picks\":\n",
    "            picks_row = row\n",
    "            break\n",
    "    \n",
    "    if not picks_row:\n",
    "        print(\"[DEBUG] No picks row found.\")\n",
    "        return []\n",
    "    \n",
    "    picks_div = picks_row.find(\"div\", class_=\"col-10\")\n",
    "    if not picks_div:\n",
    "        print(\"[DEBUG] No col-10 div next to 'Picks'.\")\n",
    "        return []\n",
    "    \n",
    "    champion_names = []\n",
    "    \n",
    "    # Collect all <a> tags that lead to \"/champion/champion-stats/...\".\n",
    "    # Some might not have `class=\"black_link\"`, so we ignore the class.\n",
    "    anchor_tags = picks_div.find_all(\"a\", href=re.compile(r\"/champion/champion-stats/\"))\n",
    "    \n",
    "    for link in anchor_tags:\n",
    "        relative_url = link.get(\"href\", \"\")\n",
    "        full_champ_url = urljoin(BASE_URL, relative_url.replace(\"..\", \"\"))\n",
    "        champion_names.append(get_champion_name(full_champ_url))\n",
    "    \n",
    "    return champion_names\n",
    "\n",
    "def scrape_lec_winter_2025_test(limit: int = 1):\n",
    "    \"\"\"\n",
    "    Scrape the first `limit` matches from the LEC Winter 2025 page on gol.gg\n",
    "    and return a list of dictionaries. This version focuses on debugging champion picks.\n",
    "    \"\"\"\n",
    "    print(f\"[DEBUG] Fetching tournament match list: {TOURNAMENT_URL}\")\n",
    "    resp = requests.get(TOURNAMENT_URL, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    rows = soup.select(\"table.table_list tbody tr\")\n",
    "    print(f\"[DEBUG] Found {len(rows)} rows in the match list.\")\n",
    "    \n",
    "    scraped_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for row in rows:\n",
    "        if count >= limit:\n",
    "            break\n",
    "        \n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) < 7:\n",
    "            continue\n",
    "        \n",
    "        patch = cells[5].get_text(strip=True)\n",
    "        \n",
    "        # Link to the detailed game page\n",
    "        game_link_rel = cells[0].find(\"a\")[\"href\"]  # e.g. \"../game/stats/62978/page-game/\"\n",
    "        game_url = urljoin(BASE_URL, game_link_rel.replace(\"..\", \"\"))\n",
    "        \n",
    "        print(f\"\\n[DEBUG] Scraping game page: {game_url}\")\n",
    "        time.sleep(0.5)\n",
    "        game_resp = requests.get(game_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        game_soup = BeautifulSoup(game_resp.text, \"html.parser\")\n",
    "        \n",
    "        sides = game_soup.select(\"div.col-12.col-sm-6\")\n",
    "        if len(sides) < 2:\n",
    "            print(\"[DEBUG] Could not find both blue and red side divs.\")\n",
    "            continue\n",
    "        \n",
    "        # -- Blue team info\n",
    "        blue_header = sides[0].select_one(\".blue-line-header\")\n",
    "        blue_team_name = \"UnknownBlue\"\n",
    "        blue_win = False\n",
    "        if blue_header:\n",
    "            header_text = blue_header.get_text(strip=True)\n",
    "            # Usually looks like: \"Team Heretics - LOSS\" or \"... - WIN\"\n",
    "            if \"- WIN\" in header_text:\n",
    "                blue_win = True\n",
    "                blue_team_name = header_text.replace(\"- WIN\", \"\").strip()\n",
    "            elif \"- LOSS\" in header_text:\n",
    "                blue_team_name = header_text.replace(\"- LOSS\", \"\").strip()\n",
    "        \n",
    "        # -- Red team info\n",
    "        red_header = sides[1].select_one(\".red-line-header\")\n",
    "        red_team_name = \"UnknownRed\"\n",
    "        if red_header:\n",
    "            header_text = red_header.get_text(strip=True)\n",
    "            if \"- WIN\" in header_text:\n",
    "                red_team_name = header_text.replace(\"- WIN\", \"\").strip()\n",
    "            elif \"- LOSS\" in header_text:\n",
    "                red_team_name = header_text.replace(\"- LOSS\", \"\").strip()\n",
    "        \n",
    "        # -- Gather picks\n",
    "        blue_champs = get_picked_champions(sides[0])\n",
    "        red_champs = get_picked_champions(sides[1])\n",
    "        \n",
    "        print(f\"[DEBUG] Blue champs: {blue_champs}\")\n",
    "        print(f\"[DEBUG] Red champs: {red_champs}\")\n",
    "        \n",
    "        game_data = {\n",
    "            \"patch\": patch,\n",
    "            \"blueTeamName\": blue_team_name,\n",
    "            \"redTeamName\":  red_team_name,\n",
    "            \"blueTeamWin\":  blue_win,\n",
    "            \"blueChamps\":   blue_champs,\n",
    "            \"redChamps\":    red_champs\n",
    "        }\n",
    "        \n",
    "        scraped_data.append(game_data)\n",
    "        count += 1\n",
    "    \n",
    "    return scraped_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = scrape_lec_winter_2025_test(limit=100)\n",
    "    print(\"\\n[DEBUG] Final scraped data:\")\n",
    "    for item in results:\n",
    "        print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from utils.rl.champions import Champion  # your champion Enum\n",
    "from fuzzywuzzy import process  # or from rapidfuzz import process\n",
    "\n",
    "# Pre-build a lookup dict for exact matches: {\"Ornn\": Champion.ORNN, \"Gnar\": Champion.GNAR, ...}\n",
    "# Key = champion.display_name with punctuation removed / standardized (e.g. \"KSante\" -> \"KSante\", \"K'Sante\" -> \"KSante\")\n",
    "lookup_dict = {}\n",
    "\n",
    "\n",
    "# We'll create a function that normalizes strings (remove apostrophes, etc.)\n",
    "def normalize_name(name: str) -> str:\n",
    "    # Example: \"K'Sante\" -> \"KSante\", \"Wukong\" -> \"Wukong\"\n",
    "    # Just remove apostrophes/spaces. Adjust as needed.\n",
    "    return name.replace(\"'\", \"\").replace(\" \", \"\").lower()\n",
    "\n",
    "\n",
    "for champ in Champion:\n",
    "    norm_display = normalize_name(champ.display_name)\n",
    "    lookup_dict[norm_display] = champ\n",
    "\n",
    "\n",
    "def map_champion_name_to_id(website_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Try to map champion name from Gol.GG (e.g. \"KSante\") to the official champion id\n",
    "    in the `Champion` enum. Uses exact match if possible, otherwise fuzzy matching.\n",
    "    \"\"\"\n",
    "    normalized = normalize_name(website_name)\n",
    "    if normalized in lookup_dict:\n",
    "        return lookup_dict[normalized].id\n",
    "\n",
    "    # Otherwise, do a fuzzy match across known keys\n",
    "    # This helps if the site name is slightly off, e.g. \"Ksante\" vs \"KSante\"\n",
    "    best_match, score = process.extractOne(normalized, list(lookup_dict.keys()))\n",
    "    if score > 50:  # arbitrary cutoff\n",
    "        return lookup_dict[best_match].id\n",
    "\n",
    "    # If still no match, return -1 or some sentinel:\n",
    "    return -1  # \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def predict_blue_win_probability(\n",
    "    blue_champs: List[str],\n",
    "    red_champs: List[str],\n",
    "    patch: str,\n",
    "    api_key: str = \"YOUR_API_KEY\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calls the /predict endpoint with the champion IDs and patch.\n",
    "    Returns the predicted probability that the blue side wins (0..1).\n",
    "    \"\"\"\n",
    "    # Convert champion names -> champion IDs\n",
    "    # Gol.GG lists champions in an order: Blue side [champ1..5], Red side [champ6..10].\n",
    "    # The model expects champion_ids in a single list: [blue1,blue2,blue3,blue4,blue5, red1,red2,red3,red4,red5]\n",
    "    champion_ids = []\n",
    "    for cname in blue_champs:\n",
    "        champion_ids.append(map_champion_name_to_id(cname))\n",
    "    for cname in red_champs:\n",
    "        champion_ids.append(map_champion_name_to_id(cname))\n",
    "\n",
    "    payload = {\n",
    "        \"champion_ids\": champion_ids,\n",
    "        \"numerical_elo\":  0, # 0 for master +\n",
    "        \"patch\": patch,\n",
    "    }\n",
    "    print(f\"payload: {payload}\")\n",
    "    headers = {\"Content-Type\": \"application/json\", \"X-API-Key\": api_key}\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(\n",
    "            \"https://loldraftai.com/api/predict\",\n",
    "            json=payload,\n",
    "            headers=headers,\n",
    "            timeout=10,\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        # data is shape: {\"win_probability\": 0.62, ...}\n",
    "        return data[\"win_probability\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling model API: {e}\")\n",
    "        return 0.5  # fallback: 50% if error\n",
    "\n",
    "\n",
    "def create_csv(matches: List[Dict], output_csv: str = \"matches_predictions.csv\"):\n",
    "    \"\"\"\n",
    "    `matches` is a list of dicts like:\n",
    "    {\n",
    "      \"patch\": \"15.2\",\n",
    "      \"blueTeamName\": \"SK Gaming\",\n",
    "      \"redTeamName\": \"Fnatic\",\n",
    "      \"blueTeamWin\": False,\n",
    "      \"blueChamps\": [\"Ornn\", \"Diana\", \"Corki\", \"Varus\", \"Poppy\"],\n",
    "      \"redChamps\": [\"Gnar\", \"Zyra\", \"Yone\", \"Ezreal\", \"Rell\"]\n",
    "    }\n",
    "\n",
    "    This writes out a CSV with the final predictions.\n",
    "    \"\"\"\n",
    "    with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        fieldnames = [\n",
    "            \"patch\",\n",
    "            \"blueTeamName\",\n",
    "            \"redTeamName\",\n",
    "            \"blueChamps\",\n",
    "            \"redChamps\",\n",
    "            \"predictedWinPct\",  # model's percentage for Blue side\n",
    "            \"modelWasCorrect\",\n",
    "        ]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for match in matches:\n",
    "            patch = match[\"patch\"]\n",
    "            blueTeamName = match[\"blueTeamName\"]\n",
    "            redTeamName = match[\"redTeamName\"]\n",
    "            blueTeamWin = match[\"blueTeamWin\"]  # True or False\n",
    "            blueChamps = match[\"blueChamps\"]\n",
    "            redChamps = match[\"redChamps\"]\n",
    "\n",
    "            # Call our function to get the predicted probability\n",
    "            win_prob = predict_blue_win_probability(blueChamps, redChamps, patch)\n",
    "            print(f\"win_prob: {win_prob}\")\n",
    "\n",
    "            # Convert to percentage\n",
    "            predicted_pct = round(win_prob * 100, 2)\n",
    "\n",
    "            # The model is \"correct\" if it predicted > 50% for the team that actually won\n",
    "            # (i.e., if blueTeamWin == True, we want win_prob >= 0.5)\n",
    "            # if blueTeamWin == False, we want win_prob < 0.5\n",
    "            predicted_winner_is_blue = win_prob >= 0.5\n",
    "            model_correct = predicted_winner_is_blue == blueTeamWin\n",
    "\n",
    "            # Save row\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"patch\": patch,\n",
    "                    \"blueTeamName\": blueTeamName,\n",
    "                    \"redTeamName\": redTeamName,\n",
    "                    \"blueChamps\": \"|\".join(blueChamps),  # or a comma, your choice\n",
    "                    \"redChamps\": \"|\".join(redChamps),\n",
    "                    \"predictedWinPct\": predicted_pct,\n",
    "                    \"modelWasCorrect\": model_correct,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Optional: just being polite to your API\n",
    "            time.sleep(0.3)\n",
    "\n",
    "    print(f\"Done. Predictions written to {output_csv}\")\n",
    "\n",
    "\n",
    "create_csv(results, output_csv=\"matches_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
