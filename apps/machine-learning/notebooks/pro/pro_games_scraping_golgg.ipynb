{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "\n",
    "def get_golgg_game_stats(game_id: int) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scrape game statistics from gol.gg for a specific game ID.\n",
    "\n",
    "    Args:\n",
    "        game_id: The unique identifier for the game on gol.gg\n",
    "                (e.g., 64750 from https://gol.gg/game/stats/64750/page-game/)\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the parsed game data, or None if the request failed\n",
    "\n",
    "    Example:\n",
    "        game_data = get_golgg_game_stats(64750)\n",
    "    \"\"\"\n",
    "    # Construct the URL with the provided game ID\n",
    "    url = f\"https://gol.gg/game/stats/{game_id}/page-game/\"\n",
    "\n",
    "    user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36\"\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "    # Send HTTP request\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise exception for 4XX/5XX responses\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_golgg_game_stats(64750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_duration(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the game duration from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object containing the parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        Game duration as a string (e.g., \"44:59\") or None if not found\n",
    "    \"\"\"\n",
    "    # Find the div containing \"Game Time\" text\n",
    "    game_time_div = soup.find(string=\"Game Time\")\n",
    "\n",
    "    if game_time_div:\n",
    "        # Navigate to the h1 element containing the duration\n",
    "        h1_element = game_time_div.find_next(\"h1\")\n",
    "        if h1_element:\n",
    "            return h1_element.text.strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_game_duration(duration: str) -> int:\n",
    "    \"\"\"\n",
    "    Parse game duration string into total seconds.\n",
    "\n",
    "    Args:\n",
    "        duration: Game duration string (e.g., \"44:59\")\n",
    "\n",
    "    Returns:\n",
    "        Duration in seconds (e.g., 2699)\n",
    "    \"\"\"\n",
    "    total_seconds = 0\n",
    "\n",
    "    if duration:\n",
    "        time_parts = duration.split(\":\")\n",
    "        if len(time_parts) == 2:\n",
    "            minutes = int(time_parts[0])\n",
    "            seconds = int(time_parts[1])\n",
    "            total_seconds = minutes * 60 + seconds\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "\n",
    "def get_game_patch(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the game patch version from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object containing the parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        Game patch as a string (e.g., \"v15.4\") or None if not found\n",
    "    \"\"\"\n",
    "    # Find the div containing \"Game Time\" text\n",
    "    game_time_div = soup.find(string=\"Game Time\")\n",
    "\n",
    "    if game_time_div:\n",
    "        # Navigate to the parent elements to find the patch div\n",
    "        row_div = game_time_div.find_parent(\"div\").find_parent(\"div\").find_parent(\"div\")\n",
    "        if row_div:\n",
    "            # Find the div with class \"col-3 text-right\" which contains the patch\n",
    "            patch_div = row_div.find(\"div\", class_=\"col-3 text-right\")\n",
    "            if patch_div:\n",
    "                return patch_div.text.strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_game_version(patch: str) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Parse game patch version into major and minor components.\n",
    "\n",
    "    Args:\n",
    "        patch: Game patch version string (e.g., \"v15.4\")\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (major_patch, minor_patch)\n",
    "    \"\"\"\n",
    "    major_patch = 0\n",
    "    minor_patch = 0\n",
    "\n",
    "    if patch and patch.startswith(\"v\"):\n",
    "        version_parts = patch[1:].split(\".\")\n",
    "        if len(version_parts) >= 1:\n",
    "            major_patch = int(version_parts[0])\n",
    "        if len(version_parts) >= 2:\n",
    "            minor_patch = int(version_parts[1])\n",
    "\n",
    "    return major_patch, minor_patch\n",
    "\n",
    "\n",
    "patch = get_game_patch(soup)\n",
    "duration = get_game_duration(soup)\n",
    "\n",
    "print(patch, duration)\n",
    "\n",
    "print(parse_game_version(patch))\n",
    "print(parse_game_duration(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_info(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract team information from the parsed HTML, including team names and winner.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object containing the parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - blue_team: Name of the blue team\n",
    "        - red_team: Name of the red team\n",
    "        - blue_won: 1 if blue team won, 0 if lost\n",
    "    \"\"\"\n",
    "    team_info = {\"blue_team\": None, \"red_team\": None, \"blue_won\": None}\n",
    "\n",
    "    # Find the blue team div\n",
    "    blue_div = soup.find(\"div\", class_=\"blue-line-header\")\n",
    "    if blue_div:\n",
    "        # Extract team name from the anchor tag\n",
    "        blue_team_anchor = blue_div.find(\"a\")\n",
    "        if blue_team_anchor:\n",
    "            team_info[\"blue_team\"] = blue_team_anchor.text.strip()\n",
    "\n",
    "        # Check if blue team won\n",
    "        blue_result = blue_div.text.strip()\n",
    "        team_info[\"blue_won\"] = 1 if \"- WIN\" in blue_result else 0\n",
    "\n",
    "    # Find the red team div\n",
    "    red_div = soup.find(\"div\", class_=\"red-line-header\")\n",
    "    if red_div:\n",
    "        # Extract team name from the anchor tag\n",
    "        red_team_anchor = red_div.find(\"a\")\n",
    "        if red_team_anchor:\n",
    "            team_info[\"red_team\"] = red_team_anchor.text.strip()\n",
    "\n",
    "    return team_info\n",
    "\n",
    "\n",
    "team_info = get_team_info(soup)\n",
    "\n",
    "print(team_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_champion_names(soup: BeautifulSoup) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract the champion names from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object containing the parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        List of 10 champion names in order: blue team (top to support) followed by red team (top to support)\n",
    "    \"\"\"\n",
    "    champion_names = []\n",
    "\n",
    "    # Find the tables with class \"playersInfosLine\"\n",
    "    player_tables = soup.find_all(\"table\", class_=\"playersInfosLine\")\n",
    "\n",
    "    # Process each table (blue team and red team)\n",
    "    for table in player_tables:\n",
    "        # Find all rows in the table (skip the header row)\n",
    "        rows = table.find_all(\"tr\")\n",
    "        # Skip the header row (first row)\n",
    "        for row in rows[1:]:  # Start from index 1 to skip header\n",
    "            # Find the champion image element\n",
    "            champion_img = row.find(\"img\", class_=\"champion_icon rounded-circle\")\n",
    "            if champion_img:\n",
    "                # Get the champion name from the alt attribute\n",
    "                champion_name = champion_img.get(\"alt\")\n",
    "                champion_names.append(champion_name)\n",
    "\n",
    "    return champion_names\n",
    "\n",
    "\n",
    "champion_names = get_champion_names(soup)\n",
    "\n",
    "print(champion_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Any\n",
    "from utils.match_prediction.champions import Champion\n",
    "import difflib  # For fuzzy string matching\n",
    "\n",
    "\n",
    "def map_champion_names_to_ids(champion_names: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Map champion names to their corresponding IDs using the Champion enum with fuzzy matching.\n",
    "\n",
    "    Args:\n",
    "        champion_names: List of champion names\n",
    "\n",
    "    Returns:\n",
    "        List of champion IDs in the same order\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If a champion name cannot be mapped to an ID\n",
    "    \"\"\"\n",
    "    champion_ids = []\n",
    "\n",
    "    # Create a mapping of display names to champion IDs for quick lookup\n",
    "    name_to_id_map = {champion.display_name: champion.id for champion in Champion}\n",
    "    all_champion_names = list(name_to_id_map.keys())\n",
    "\n",
    "    # Create a reverse mapping for easier lookup (normalized name -> display name)\n",
    "    normalized_name_map = {}\n",
    "    for display_name in all_champion_names:\n",
    "        # Store both the lowercase version and a version with apostrophes removed\n",
    "        normalized_name_map[display_name.lower()] = display_name\n",
    "        normalized_name_map[display_name.lower().replace(\"'\", \"\")] = display_name\n",
    "\n",
    "    # Special case handling for known mismatches\n",
    "    special_cases = {\"nunu\": \"Nunu & Willump\"}\n",
    "\n",
    "    for name in champion_names:\n",
    "        champion_id = None\n",
    "\n",
    "        # Handle special cases\n",
    "        if name.lower() in special_cases:\n",
    "            name = special_cases[name.lower()]\n",
    "\n",
    "        # Try direct lookup first\n",
    "        if name in name_to_id_map:\n",
    "            champion_id = name_to_id_map[name]\n",
    "\n",
    "        # Try normalized lookup (handles case differences and missing apostrophes)\n",
    "        elif name.lower() in normalized_name_map:\n",
    "            display_name = normalized_name_map[name.lower()]\n",
    "            champion_id = name_to_id_map[display_name]\n",
    "\n",
    "        # Try without apostrophes\n",
    "        elif name.lower().replace(\"'\", \"\") in normalized_name_map:\n",
    "            display_name = normalized_name_map[name.lower().replace(\"'\", \"\")]\n",
    "            champion_id = name_to_id_map[display_name]\n",
    "\n",
    "        # If still not found, try fuzzy matching\n",
    "        if champion_id is None:\n",
    "            # Get the closest match using difflib\n",
    "            closest_matches = difflib.get_close_matches(\n",
    "                name, all_champion_names, n=1, cutoff=0.6\n",
    "            )\n",
    "\n",
    "            if closest_matches:\n",
    "                closest_match = closest_matches[0]\n",
    "                champion_id = name_to_id_map[closest_match]\n",
    "                print(\n",
    "                    f\"Warning: Using fuzzy match for '{name}' -> '{closest_match}' (ID: {champion_id})\"\n",
    "                )\n",
    "            else:\n",
    "                # If no match found, raise an error\n",
    "                raise ValueError(f\"Could not map champion name '{name}' to an ID.\")\n",
    "\n",
    "        champion_ids.append(champion_id)\n",
    "\n",
    "    return champion_ids\n",
    "\n",
    "\n",
    "champion_ids = map_champion_names_to_ids(champion_names)\n",
    "\n",
    "print(champion_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tournament_name(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the tournament name from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object containing the parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        Tournament name as a string or None if not found\n",
    "    \"\"\"\n",
    "    # Find the anchor tag with the specific href pattern\n",
    "    tournament_anchor = soup.find(\n",
    "        \"a\", href=lambda href: href and \"../tournament/tournament-stats\" in href\n",
    "    )\n",
    "\n",
    "    if tournament_anchor:\n",
    "        return tournament_anchor.text.strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "tournament_name = get_tournament_name(soup)\n",
    "print(tournament_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timeline scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def get_timeline_soup(game_id: str) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Fetches the timeline page for a specific game and returns its BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        game_id: The ID of the game to fetch\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup object of the timeline page\n",
    "    \"\"\"\n",
    "    url = f\"https://gol.gg/game/stats/{game_id}/page-timeline/\"\n",
    "    user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36\"\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "    # Send HTTP request\n",
    "    # Send HTTP request\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise exception for 4XX/5XX responses\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def js_to_json(js_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert JavaScript object notation to valid JSON.\n",
    "\n",
    "    Args:\n",
    "        js_str: JavaScript object string\n",
    "\n",
    "    Returns:\n",
    "        Valid JSON string\n",
    "    \"\"\"\n",
    "    # Add quotes to property names\n",
    "    js_str = re.sub(r\"([{,])\\s*(\\w+):\", r'\\1\"\\2\":', js_str)\n",
    "\n",
    "    # Replace single quotes with double quotes, but only for strings\n",
    "    # First, temporarily replace any escaped single quotes\n",
    "    js_str = js_str.replace(\"\\\\'\", \"___ESCAPED_QUOTE___\")\n",
    "\n",
    "    # Then handle the actual quotes\n",
    "    in_string = False\n",
    "    result = []\n",
    "    for char in js_str:\n",
    "        if char == \"'\" and not in_string:\n",
    "            result.append('\"')\n",
    "            in_string = True\n",
    "        elif char == \"'\" and in_string:\n",
    "            result.append('\"')\n",
    "            in_string = False\n",
    "        else:\n",
    "            result.append(char)\n",
    "    js_str = \"\".join(result)\n",
    "\n",
    "    # Restore escaped quotes\n",
    "    js_str = js_str.replace(\"___ESCAPED_QUOTE___\", \"\\\\'\")\n",
    "\n",
    "    # Remove trailing commas in arrays and objects\n",
    "    js_str = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", js_str)\n",
    "\n",
    "    return js_str\n",
    "\n",
    "\n",
    "def create_empty_timeline_data() -> Dict[str, Optional[float]]:\n",
    "    \"\"\"Helper function to create a dictionary with None values for all timeline metrics\"\"\"\n",
    "    result = {}\n",
    "    timestamps = [\"900000\", \"1200000\"]\n",
    "    for team_id in [\"100\", \"200\"]:\n",
    "        for position in [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"]:\n",
    "            for timestamp in timestamps:\n",
    "                result[f\"team_{team_id}_{position}_totalGold_at_{timestamp}\"] = None\n",
    "                result[f\"team_{team_id}_{position}_creepScore_at_{timestamp}\"] = None\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_timeline_data(soup: BeautifulSoup) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Extracts gold and CS data from the timeline page.\n",
    "    Returns None values for missing data instead of raising exceptions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the script containing the data\n",
    "        script_content = soup.find(\"script\", string=lambda t: t and \"golddatas\" in t)\n",
    "        if not script_content:\n",
    "            print(\"Timeline data not found in page\")\n",
    "            return create_empty_timeline_data()\n",
    "\n",
    "        # Extract the data objects using regex\n",
    "        gold_match = re.search(\n",
    "            r\"var\\s+golddatas\\s*=\\s*({.*?});\", script_content.string, re.DOTALL\n",
    "        )\n",
    "        cs_match = re.search(\n",
    "            r\"var\\s+csdatas\\s*=\\s*({.*?});\", script_content.string, re.DOTALL\n",
    "        )\n",
    "\n",
    "        if not gold_match or not cs_match:\n",
    "            print(\"Could not parse gold or CS data\")\n",
    "            return create_empty_timeline_data()\n",
    "\n",
    "        # Parse the JSON-like strings\n",
    "        # Replace single quotes with double quotes for valid JSON\n",
    "        # Convert JavaScript objects to valid JSON\n",
    "        json_gold = js_to_json(gold_match.group(1))\n",
    "        json_cs = js_to_json(cs_match.group(1))\n",
    "        gold_data = json.loads(json_gold)\n",
    "        cs_data = json.loads(json_cs)\n",
    "\n",
    "        # Initialize result dictionary\n",
    "        result = {}\n",
    "\n",
    "        # Define positions mapping (gol.gg uses different names)\n",
    "        position_mapping = {\n",
    "            \"TOP\": \"TOP\",\n",
    "            \"JGL\": \"JUNGLE\",\n",
    "            \"MID\": \"MIDDLE\",\n",
    "            \"BOT\": \"BOTTOM\",\n",
    "            \"SPT\": \"UTILITY\",\n",
    "        }\n",
    "\n",
    "        # Extract data for minutes 15 (index 15) and 20 (index 20)\n",
    "        timestamps = [\"900000\", \"1200000\"]  # 15 min and 20 min\n",
    "        time_indices = [15, 20]\n",
    "\n",
    "        # First 5 datasets are team 100, last 5 are team 200\n",
    "        for team_idx, team_id in enumerate([\"100\", \"200\"]):\n",
    "            base_offset = team_idx * 5\n",
    "            for dataset_idx, dataset in enumerate(\n",
    "                gold_data[\"datasets\"][base_offset : base_offset + 5]\n",
    "            ):\n",
    "                position = position_mapping[dataset[\"label\"]]\n",
    "\n",
    "                # Get gold data\n",
    "                for time_idx, timestamp in zip(time_indices, timestamps):\n",
    "                    gold_value = dataset[\"data\"][time_idx]\n",
    "                    result[f\"team_{team_id}_{position}_totalGold_at_{timestamp}\"] = (\n",
    "                        gold_value\n",
    "                    )\n",
    "\n",
    "                # Get CS data\n",
    "                cs_dataset = cs_data[\"datasets\"][base_offset + dataset_idx]\n",
    "                for time_idx, timestamp in zip(time_indices, timestamps):\n",
    "                    cs_value = cs_dataset[\"data\"][time_idx]\n",
    "                    result[f\"team_{team_id}_{position}_creepScore_at_{timestamp}\"] = (\n",
    "                        cs_value\n",
    "                    )\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting timeline data: {e}\")\n",
    "        return create_empty_timeline_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_timeline_soup(64750)\n",
    "timeline_data = extract_timeline_data(soup)\n",
    "print(timeline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_kda_data() -> Dict[str, Optional[int]]:\n",
    "    \"\"\"Helper function to create a dictionary with None values for all KDA metrics\"\"\"\n",
    "    stats = {}\n",
    "    for team_id in [\"100\", \"200\"]:\n",
    "        for position in [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"]:\n",
    "            for stat in [\"kills\", \"deaths\", \"assists\"]:\n",
    "                for timestamp in [\"900000\", \"1200000\"]:\n",
    "                    stats[f\"team_{team_id}_{position}_{stat}_at_{timestamp}\"] = None\n",
    "    return stats\n",
    "\n",
    "\n",
    "def parse_timeline_kda(\n",
    "    soup: BeautifulSoup, champion_positions: Dict[str, str]\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extract KDA stats from timeline events.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object of the timeline page\n",
    "        champion_positions: Dictionary mapping champion names to their positions\n",
    "            e.g. {\"MissFortune\": \"BOTTOM\", \"Maokai\": \"JUNGLE\", ...}\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing KDA stats for each position and team at 15 and 20 minutes\n",
    "        Format: {\n",
    "            'team_100_TOP_kills_at_900000': value,\n",
    "            'team_100_TOP_deaths_at_900000': value,\n",
    "            'team_100_TOP_assists_at_900000': value,\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Initialize KDA counters for each position and team\n",
    "    stats = {}\n",
    "    for team_id in [\"100\", \"200\"]:\n",
    "        for position in [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"]:\n",
    "            for stat in [\"kills\", \"deaths\", \"assists\"]:\n",
    "                for timestamp in [\"900000\", \"1200000\"]:  # 15 min and 20 min\n",
    "                    stats[f\"team_{team_id}_{position}_{stat}_at_{timestamp}\"] = 0\n",
    "\n",
    "    try:\n",
    "        events_table = soup.find(\"table\", {\"class\": \"timeline\"})\n",
    "        if not events_table:\n",
    "            print(\"Could not find timeline events table\")\n",
    "            return create_empty_kda_data()\n",
    "\n",
    "        # Current KDA counts\n",
    "        current_kda = {\n",
    "            \"100\": {\n",
    "                pos: {\"kills\": 0, \"deaths\": 0, \"assists\": 0}\n",
    "                for pos in [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"]\n",
    "            },\n",
    "            \"200\": {\n",
    "                pos: {\"kills\": 0, \"deaths\": 0, \"assists\": 0}\n",
    "                for pos in [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"]\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Process each event row\n",
    "        for row in events_table.find_all(\"tr\")[1:]:  # Skip header row\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) < 5:\n",
    "                continue\n",
    "\n",
    "            # Parse timestamp\n",
    "            time_str = cols[0].text.strip()\n",
    "            if not time_str:\n",
    "                continue\n",
    "\n",
    "            minutes, seconds = map(int, time_str.split(\":\"))\n",
    "            event_time = minutes * 60 + seconds\n",
    "\n",
    "            # Check if it's a kill event\n",
    "            kill_icon = cols[4].find(\"img\", {\"src\": \"../_img/kill-icon.png\"})\n",
    "            if not kill_icon:\n",
    "                continue\n",
    "\n",
    "            # Get killer team (blue = 100, red = 200)\n",
    "            team_side = cols[1].find(\"img\")[\"src\"]\n",
    "            team_id = \"100\" if \"blueside\" in team_side else \"200\"\n",
    "\n",
    "            # Get killer champion and their position\n",
    "            # Updated selector to match height:25px in style attribute\n",
    "            killer_champ = cols[3].find(\n",
    "                \"img\",\n",
    "                {\n",
    "                    \"src\": lambda x: x and \"champions_icon\" in x,\n",
    "                    \"style\": lambda x: x and \"height:25px\" in x,\n",
    "                },\n",
    "            )\n",
    "            if not killer_champ:\n",
    "                continue\n",
    "\n",
    "            killer_name = (\n",
    "                killer_champ[\"src\"].split(\"/\")[-1].replace(\".png\", \"\")\n",
    "            )  # Extract name from src\n",
    "            # Handle special cases in champion names\n",
    "            killer_name = (\n",
    "                killer_name.replace(\"_\", \" \")\n",
    "                .replace(\"KSante\", \"K'Sante\")\n",
    "                .replace(\"KaiSa\", \"Kai'Sa\")\n",
    "                .replace(\"KhaZix\", \"Kha'Zix\")\n",
    "                .replace(\"RekSai\", \"Rek'Sai\")\n",
    "                .replace(\"VelKoz\", \"Vel'Koz\")\n",
    "            )\n",
    "            killer_position = champion_positions.get(killer_name)\n",
    "\n",
    "            # Get assist champions and their positions\n",
    "            assist_champs = cols[3].find_all(\n",
    "                \"img\",\n",
    "                {\n",
    "                    \"src\": lambda x: x and \"champions_icon\" in x,\n",
    "                    \"style\": lambda x: x and \"height:18px\" in x,\n",
    "                },\n",
    "            )\n",
    "            assist_positions = []\n",
    "            for champ in assist_champs:\n",
    "                assist_name = champ[\"src\"].split(\"/\")[-1].replace(\".png\", \"\")\n",
    "                # Handle special cases in champion names\n",
    "                assist_name = (\n",
    "                    assist_name.replace(\"_\", \" \")\n",
    "                    .replace(\"KSante\", \"K'Sante\")\n",
    "                    .replace(\"KaiSa\", \"Kai'Sa\")\n",
    "                    .replace(\"KhaZix\", \"Kha'Zix\")\n",
    "                    .replace(\"RekSai\", \"Rek'Sai\")\n",
    "                    .replace(\"VelKoz\", \"Vel'Koz\")\n",
    "                )\n",
    "                assist_position = champion_positions.get(assist_name)\n",
    "                if assist_position:\n",
    "                    assist_positions.append(assist_position)\n",
    "\n",
    "            # Get target champion and position\n",
    "            target_champ = cols[5].find(\n",
    "                \"img\", {\"src\": lambda x: x and \"champions_icon\" in x}\n",
    "            )\n",
    "            if not target_champ:\n",
    "                continue\n",
    "\n",
    "            target_name = target_champ[\"src\"].split(\"/\")[-1].replace(\".png\", \"\")\n",
    "            # Handle special cases in champion names\n",
    "            target_name = (\n",
    "                target_name.replace(\"_\", \" \")\n",
    "                .replace(\"KSante\", \"K'Sante\")\n",
    "                .replace(\"KaiSa\", \"Kai'Sa\")\n",
    "                .replace(\"KhaZix\", \"Kha'Zix\")\n",
    "                .replace(\"RekSai\", \"Rek'Sai\")\n",
    "                .replace(\"VelKoz\", \"Vel'Koz\")\n",
    "            )\n",
    "            target_position = champion_positions.get(target_name)\n",
    "            target_team = \"200\" if team_id == \"100\" else \"100\"\n",
    "\n",
    "            # Update KDA counts\n",
    "            if killer_position:\n",
    "                current_kda[team_id][killer_position][\"kills\"] += 1\n",
    "            if target_position:\n",
    "                current_kda[target_team][target_position][\"deaths\"] += 1\n",
    "            for assist_pos in assist_positions:\n",
    "                if assist_pos:\n",
    "                    current_kda[team_id][assist_pos][\"assists\"] += 1\n",
    "\n",
    "            # Update stats at 15 min (900000) if we just passed it\n",
    "            if event_time <= 900 and (minutes == 15 or minutes == 14):\n",
    "                for t_id in [\"100\", \"200\"]:\n",
    "                    for pos in [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"]:\n",
    "                        for stat in [\"kills\", \"deaths\", \"assists\"]:\n",
    "                            stats[f\"team_{t_id}_{pos}_{stat}_at_900000\"] = current_kda[\n",
    "                                t_id\n",
    "                            ][pos][stat]\n",
    "\n",
    "            # Update stats at 20 min (1200000) if we just passed it\n",
    "            if event_time <= 1200 and (minutes == 20 or minutes == 19):\n",
    "                for t_id in [\"100\", \"200\"]:\n",
    "                    for pos in [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"]:\n",
    "                        for stat in [\"kills\", \"deaths\", \"assists\"]:\n",
    "                            stats[f\"team_{t_id}_{pos}_{stat}_at_1200000\"] = current_kda[\n",
    "                                t_id\n",
    "                            ][pos][stat]\n",
    "\n",
    "        # If we never hit 15 or 20 minutes, use the final counts\n",
    "        for timestamp in [\"900000\", \"1200000\"]:\n",
    "            for t_id in [\"100\", \"200\"]:\n",
    "                for pos in [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"]:\n",
    "                    for stat in [\"kills\", \"deaths\", \"assists\"]:\n",
    "                        key = f\"team_{t_id}_{pos}_{stat}_at_{timestamp}\"\n",
    "                        if stats[key] == 0:\n",
    "                            stats[key] = current_kda[t_id][pos][stat]\n",
    "\n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing timeline KDA: {e}\")\n",
    "        return create_empty_kda_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_positions = {}\n",
    "for idx, position in enumerate([\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"] * 2):\n",
    "    champion_positions[champion_names[idx]] = position\n",
    "print(champion_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kda_stats = parse_timeline_kda(soup, champion_positions)\n",
    "print(kda_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import random\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "\n",
    "def get_game_data(game_id: int) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get complete game data including timeline stats for a specific game ID.\n",
    "\n",
    "    Args:\n",
    "        game_id: The game ID to fetch data for\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing all game data or None if fetching failed\n",
    "    \"\"\"\n",
    "    # Get main game page data\n",
    "    main_soup = get_golgg_game_stats(game_id)\n",
    "    if main_soup is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Extract basic game data\n",
    "        patch = get_game_patch(main_soup)\n",
    "        if not patch:\n",
    "            return None\n",
    "\n",
    "        major_patch, minor_patch = parse_game_version(patch)\n",
    "        duration_str = get_game_duration(main_soup)\n",
    "        duration_seconds = parse_game_duration(duration_str) if duration_str else 0\n",
    "        team_info = get_team_info(main_soup)\n",
    "        champion_names = get_champion_names(main_soup)\n",
    "        tournament_name = get_tournament_name(main_soup)\n",
    "\n",
    "        # Map champion names to IDs and positions\n",
    "        champion_ids = map_champion_names_to_ids(champion_names)\n",
    "        champion_positions = {}\n",
    "        for idx, position in enumerate(\n",
    "            [\"TOP\", \"JUNGLE\", \"MIDDLE\", \"BOTTOM\", \"UTILITY\"] * 2\n",
    "        ):\n",
    "            champion_positions[champion_names[idx]] = position\n",
    "\n",
    "        # Get timeline data\n",
    "        timeline_soup = get_timeline_soup(game_id)\n",
    "        if timeline_soup is None:\n",
    "            return None\n",
    "\n",
    "        # Extract timeline stats\n",
    "        timeline_gold_cs = extract_timeline_data(timeline_soup)\n",
    "        timeline_kda = parse_timeline_kda(timeline_soup, champion_positions)\n",
    "\n",
    "        # Combine all data\n",
    "        game_data = {\n",
    "            \"golgg_id\": game_id,\n",
    "            \"champion_ids\": champion_ids,\n",
    "            \"gameVersionMajorPatch\": major_patch,\n",
    "            \"gameVersionMinorPatch\": minor_patch,\n",
    "            \"gameDuration\": duration_seconds,\n",
    "            \"blueTeamName\": team_info.get(\"blue_team\", \"\"),\n",
    "            \"redTeamName\": team_info.get(\"red_team\", \"\"),\n",
    "            \"tournamentName\": tournament_name or \"\",\n",
    "            \"team_100_win\": team_info.get(\"blue_won\", None),\n",
    "        }\n",
    "\n",
    "        # Add timeline stats\n",
    "        game_data.update(timeline_gold_cs)\n",
    "        game_data.update(timeline_kda)\n",
    "\n",
    "        return game_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing game ID {game_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_golgg_games(\n",
    "    start_game_id: int,\n",
    "    output_file_path: str,\n",
    "    min_major_version: int = 14,\n",
    "    request_delay: Tuple[float, float] = (1.0, 3.0),\n",
    "    limit: int = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape game data from gol.gg starting from a specific game ID and working backwards.\n",
    "    Save the results to a parquet file.\n",
    "\n",
    "    Args:\n",
    "        start_game_id: The game ID to start scraping from\n",
    "        output_file_path: Path where the parquet file will be saved\n",
    "        min_major_version: Minimum game major version to scrape (default: 14)\n",
    "        request_delay: Tuple of (min, max) seconds to delay between requests\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing all scraped game data\n",
    "    \"\"\"\n",
    "    # Check if the parquet file already exists and load it\n",
    "    if os.path.exists(output_file_path):\n",
    "        existing_df = pd.read_parquet(output_file_path)\n",
    "        print(f\"Loaded existing data with {len(existing_df)} games\")\n",
    "\n",
    "        # Get the set of game IDs that we already have\n",
    "        existing_game_ids = set(existing_df[\"golgg_id\"].tolist())\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"golgg_id\",\n",
    "                \"champion_ids\",\n",
    "                \"gameVersionMajorPatch\",\n",
    "                \"gameVersionMinorPatch\",\n",
    "                \"gameDuration\",\n",
    "                \"blueTeamName\",\n",
    "                \"redTeamName\",\n",
    "                \"tournamentName\",\n",
    "                \"team_100_win\",\n",
    "            ]\n",
    "        )\n",
    "        existing_game_ids = set()\n",
    "        print(\"No existing data found, creating new dataset\")\n",
    "\n",
    "    # Create a list to store new game data\n",
    "    new_games_data = []\n",
    "\n",
    "    # Initialize the current game ID\n",
    "    current_id = start_game_id\n",
    "\n",
    "    try:\n",
    "        with tqdm(desc=\"Scraping games\") as pbar:\n",
    "            while current_id > 0:\n",
    "                if current_id in existing_game_ids:\n",
    "                    print(f\"Game ID {current_id} already exists in dataset. Stopping.\")\n",
    "                    break\n",
    "\n",
    "                time.sleep(random.uniform(request_delay[0], request_delay[1]))\n",
    "\n",
    "                # Use the new get_game_data function\n",
    "                game_data = get_game_data(current_id)\n",
    "\n",
    "                if game_data is None:\n",
    "                    print(\n",
    "                        f\"Game ID {current_id} not found or error occurred. Skipping.\"\n",
    "                    )\n",
    "                    current_id -= 1\n",
    "                    continue\n",
    "\n",
    "                # Check version\n",
    "                if game_data[\"gameVersionMajorPatch\"] < min_major_version:\n",
    "                    print(\n",
    "                        f\"Reached game with version below minimum {min_major_version}. Stopping.\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                new_games_data.append(game_data)\n",
    "\n",
    "                # Every 100 games, save progress\n",
    "                if len(new_games_data) % 100 == 0:\n",
    "                    # Combine existing data with new data\n",
    "                    combined_df = pd.concat(\n",
    "                        [existing_df, pd.DataFrame(new_games_data)],\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "\n",
    "                    # Save to parquet\n",
    "                    combined_df.to_parquet(output_file_path, index=False)\n",
    "                    print(f\"Saved progress: {len(combined_df)} total games\")\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"game_id\": current_id,\n",
    "                        \"version\": f\"v{game_data['gameVersionMajorPatch']}.{game_data[\"gameVersionMinorPatch\"]}\",\n",
    "                    }\n",
    "                )\n",
    "                # Move to the previous game ID\n",
    "                current_id -= 1\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping interrupted by user\")\n",
    "\n",
    "    finally:\n",
    "        # Save final results if we have new data\n",
    "        if new_games_data:\n",
    "            # Combine existing data with new data\n",
    "            final_df = pd.concat(\n",
    "                [existing_df, pd.DataFrame(new_games_data)], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # Save to parquet\n",
    "            final_df.to_parquet(output_file_path, index=False)\n",
    "            print(f\"Final dataset saved with {len(final_df)} games\")\n",
    "            return final_df\n",
    "        else:\n",
    "            print(\"No new data collected\")\n",
    "            return existing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.match_prediction import RAW_PRO_GAMES_DIR\n",
    "\n",
    "# Define starting game ID (latest game from website)\n",
    "latest_game_id = 66222  # Replace with the latest game ID you find\n",
    "\n",
    "output_file_path = os.path.join(RAW_PRO_GAMES_DIR, \"pro_games.parquet\")\n",
    "\n",
    "# Run the scraper\n",
    "pro_games_df = scrape_golgg_games(\n",
    "    start_game_id=latest_game_id,\n",
    "    output_file_path=output_file_path,\n",
    "    min_major_version=14,\n",
    "    request_delay=(\n",
    "        0.5,\n",
    "        1.0,\n",
    "    ),  # Random delay between 1-3 seconds to be respectful to the server\n",
    ")\n",
    "\n",
    "# Display summary of collected data\n",
    "print(f\"Total games collected: {len(pro_games_df)}\")\n",
    "print(f\"Unique tournaments: {pro_games_df['tournamentName'].nunique()}\")\n",
    "print(f\"Games by major version:\")\n",
    "print(pro_games_df.groupby(\"gameVersionMajorPatch\").size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_games_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fix nunu imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "def extract_failed_nunu_ids(log_file_path):\n",
    "    \"\"\"\n",
    "    Extract game IDs that failed due to Nunu mapping issues from a log file.\n",
    "\n",
    "    Args:\n",
    "        log_file_path: Path to the log file\n",
    "\n",
    "    Returns:\n",
    "        List of game IDs that failed\n",
    "    \"\"\"\n",
    "    failed_ids = []\n",
    "\n",
    "    # Regular expression to match the error lines and extract the game ID\n",
    "    pattern = r\"Error mapping champion names for game ID (\\d+): Could not map champion name 'Nunu' to an ID\\.\"\n",
    "\n",
    "    with open(log_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                game_id = int(match.group(1))\n",
    "                failed_ids.append(game_id)\n",
    "\n",
    "    return failed_ids\n",
    "\n",
    "\n",
    "log_file_path = \"/Users/loyd/nunu.txt\"\n",
    "\n",
    "# Extract the failed IDs\n",
    "failed_ids = extract_failed_nunu_ids(log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(failed_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_specific_game_ids(\n",
    "    game_ids: List[int],\n",
    "    output_file_path: str,\n",
    "    request_delay: Tuple[float, float] = (1.0, 3.0),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add specific game IDs to the dataset that may have previously failed.\n",
    "\n",
    "    Args:\n",
    "        game_ids: List of game IDs to process and add\n",
    "        output_file_path: Path to the parquet file to update\n",
    "        request_delay: Tuple of (min, max) seconds to delay between requests\n",
    "\n",
    "    Returns:\n",
    "        Updated DataFrame containing all game data\n",
    "    \"\"\"\n",
    "    # Load existing data if it exists\n",
    "    if os.path.exists(output_file_path):\n",
    "        existing_df = pd.read_parquet(output_file_path)\n",
    "        print(f\"Loaded existing data with {len(existing_df)} games\")\n",
    "\n",
    "        # Get the set of game IDs that we already have\n",
    "        existing_game_ids = set(existing_df[\"golgg_id\"].tolist())\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"golgg_id\",\n",
    "                \"champion_ids\",\n",
    "                \"gameVersionMajorPatch\",\n",
    "                \"gameVersionMinorPatch\",\n",
    "                \"gameDuration\",\n",
    "                \"blueTeamName\",\n",
    "                \"redTeamName\",\n",
    "                \"tournamentName\",\n",
    "                \"team_100_win\",\n",
    "            ]\n",
    "        )\n",
    "        existing_game_ids = set()\n",
    "        print(\"No existing data found, creating new dataset\")\n",
    "\n",
    "    # Create a list to store new game data\n",
    "    new_games_data = []\n",
    "\n",
    "    # Filter out game IDs that already exist in the dataset\n",
    "    ids_to_process = [\n",
    "        game_id for game_id in game_ids if game_id not in existing_game_ids\n",
    "    ]\n",
    "\n",
    "    if not ids_to_process:\n",
    "        print(\"All specified game IDs already exist in the dataset. Nothing to add.\")\n",
    "        return existing_df\n",
    "\n",
    "    print(f\"Processing {len(ids_to_process)} new game IDs...\")\n",
    "\n",
    "    try:\n",
    "        # Process each game ID\n",
    "        with tqdm(total=len(ids_to_process), desc=\"Adding games\") as pbar:\n",
    "            for current_id in ids_to_process:\n",
    "                # Add random delay to avoid overloading the server\n",
    "                time.sleep(random.uniform(request_delay[0], request_delay[1]))\n",
    "\n",
    "                # Get game data\n",
    "                soup = get_golgg_game_stats(current_id)\n",
    "\n",
    "                # If game not found (404/302), skip to next ID\n",
    "                if soup is None:\n",
    "                    print(f\"Game ID {current_id} not found. Skipping.\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Extract patch version\n",
    "                    patch = get_game_patch(soup)\n",
    "                    if patch:\n",
    "                        major_patch, minor_patch = parse_game_version(patch)\n",
    "                    else:\n",
    "                        # If patch info not available, skip\n",
    "                        print(f\"No patch info for game ID {current_id}. Skipping.\")\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    # Extract other game data\n",
    "                    duration_str = get_game_duration(soup)\n",
    "                    duration_seconds = (\n",
    "                        parse_game_duration(duration_str) if duration_str else 0\n",
    "                    )\n",
    "\n",
    "                    team_info = get_team_info(soup)\n",
    "                    champion_names = get_champion_names(soup)\n",
    "\n",
    "                    # Map champion names to IDs with improved function\n",
    "                    try:\n",
    "                        champion_ids = map_champion_names_to_ids(champion_names)\n",
    "                    except ValueError as e:\n",
    "                        print(\n",
    "                            f\"Error mapping champion names for game ID {current_id}: {e}. Skipping.\"\n",
    "                        )\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    tournament_name = get_tournament_name(soup)\n",
    "\n",
    "                    # Create a record for this game\n",
    "                    game_data = {\n",
    "                        \"golgg_id\": current_id,\n",
    "                        \"champion_ids\": champion_ids,\n",
    "                        \"gameVersionMajorPatch\": major_patch,\n",
    "                        \"gameVersionMinorPatch\": minor_patch,\n",
    "                        \"gameDuration\": duration_seconds,\n",
    "                        \"blueTeamName\": team_info.get(\"blue_team\", \"\"),\n",
    "                        \"redTeamName\": team_info.get(\"red_team\", \"\"),\n",
    "                        \"tournamentName\": tournament_name or \"\",\n",
    "                        \"team_100_win\": team_info.get(\"blue_won\", None),\n",
    "                        # Add default values for columns added by other scripts\n",
    "                        \"team_100_win_relabel\": -1,\n",
    "                        \"model_prediction\": -1,\n",
    "                        \"model_error\": -1,\n",
    "                    }\n",
    "\n",
    "                    # Append to our list of new games\n",
    "                    new_games_data.append(game_data)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing game ID {current_id}: {e}. Skipping.\")\n",
    "                    pbar.update(1)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user\")\n",
    "\n",
    "    finally:\n",
    "        # Save final results if we have new data\n",
    "        if new_games_data:\n",
    "            # Combine existing data with new data\n",
    "            final_df = pd.concat(\n",
    "                [existing_df, pd.DataFrame(new_games_data)], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # Save to parquet\n",
    "            final_df.to_parquet(output_file_path, index=False)\n",
    "            print(f\"Final dataset saved with {len(final_df)} games\")\n",
    "            return final_df\n",
    "        else:\n",
    "            print(\"No new data collected\")\n",
    "            return existing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.match_prediction import RAW_PRO_GAMES_DIR\n",
    "import os\n",
    "\n",
    "# Path to your dataset\n",
    "output_file_path = os.path.join(RAW_PRO_GAMES_DIR, \"pro_games.parquet\")\n",
    "\n",
    "\n",
    "# Process the failed game IDs and add them to the dataset\n",
    "updated_df = add_specific_game_ids(\n",
    "    game_ids=failed_ids,\n",
    "    output_file_path=output_file_path,\n",
    "    request_delay=(1.0, 3.0),  # Random delay between 1-3 seconds\n",
    ")\n",
    "\n",
    "# Display summary of the updated dataset\n",
    "print(f\"Total games in updated dataset: {len(updated_df)}\")\n",
    "print(f\"Unique tournaments: {updated_df['tournamentName'].nunique()}\")\n",
    "print(f\"Games by major version:\")\n",
    "print(updated_df.groupby(\"gameVersionMajorPatch\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One time update with timeline data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_existing_games_with_timeline(\n",
    "    output_file_path: str, request_delay: Tuple[float, float] = (1.0, 3.0)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Update existing games in the dataset with timeline data.\n",
    "\n",
    "    Args:\n",
    "        output_file_path: Path to the parquet file\n",
    "        request_delay: Tuple of (min, max) seconds to delay between requests\n",
    "\n",
    "    Returns:\n",
    "        Updated DataFrame\n",
    "    \"\"\"\n",
    "    # Load existing data\n",
    "    existing_df = pd.read_parquet(output_file_path)\n",
    "    print(f\"Loaded {len(existing_df)} games to update\")\n",
    "\n",
    "    # Create a new list to store updated data\n",
    "    updated_games = []\n",
    "\n",
    "    try:\n",
    "        with tqdm(total=len(existing_df), desc=\"Updating games\") as pbar:\n",
    "            for _, row in existing_df.iterrows():\n",
    "                game_id = row[\"golgg_id\"]\n",
    "                updated_row = row.to_dict()\n",
    "\n",
    "                # Add random delay\n",
    "                time.sleep(random.uniform(request_delay[0], request_delay[1]))\n",
    "\n",
    "                # Get complete game data\n",
    "                game_data = get_game_data(game_id)\n",
    "\n",
    "                if game_data is not None:\n",
    "                    # Update only the timeline-related fields and basic game data\n",
    "                    # Keep all other fields from the original row\n",
    "                    for key, value in game_data.items():\n",
    "                        # Update the row with new data\n",
    "                        updated_row[key] = value\n",
    "\n",
    "                updated_games.append(updated_row)\n",
    "\n",
    "                # Save progress every 100 games\n",
    "                if len(updated_games) % 100 == 0:\n",
    "                    temp_df = pd.DataFrame(updated_games)\n",
    "                    temp_df.to_parquet(\n",
    "                        output_file_path.replace(\".parquet\", \"_temp.parquet\"),\n",
    "                        index=False,\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"Saved progress: {len(updated_games)}/{len(existing_df)} games\"\n",
    "                    )\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nUpdate interrupted by user\")\n",
    "\n",
    "    finally:\n",
    "        if updated_games:\n",
    "            final_df = pd.DataFrame(updated_games)\n",
    "            final_df.to_parquet(output_file_path, index=False)\n",
    "            print(f\"Final dataset saved with {len(final_df)} games\")\n",
    "            return final_df\n",
    "        else:\n",
    "            return existing_df\n",
    "\n",
    "\n",
    "# Usage:\n",
    "from utils.match_prediction import RAW_PRO_GAMES_FILE\n",
    "\n",
    "updated_df = update_existing_games_with_timeline(\n",
    "    RAW_PRO_GAMES_FILE, request_delay=(0.5, 1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
