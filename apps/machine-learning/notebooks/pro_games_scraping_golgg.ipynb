{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "\n",
    "def get_golgg_game_stats(game_id: int) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Scrape game statistics from gol.gg for a specific game ID.\n",
    "\n",
    "    Args:\n",
    "        game_id: The unique identifier for the game on gol.gg\n",
    "                (e.g., 64750 from https://gol.gg/game/stats/64750/page-game/)\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the parsed game data, or None if the request failed\n",
    "\n",
    "    Example:\n",
    "        game_data = get_golgg_game_stats(64750)\n",
    "    \"\"\"\n",
    "    # Construct the URL with the provided game ID\n",
    "    url = f\"https://gol.gg/game/stats/{game_id}/page-game/\"\n",
    "\n",
    "    user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36\"\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "    # Send HTTP request\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise exception for 4XX/5XX responses\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_golgg_game_stats(64750)\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_duration(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the game duration from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object containing the parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        Game duration as a string (e.g., \"44:59\") or None if not found\n",
    "    \"\"\"\n",
    "    # Find the div containing \"Game Time\" text\n",
    "    game_time_div = soup.find(string=\"Game Time\")\n",
    "\n",
    "    if game_time_div:\n",
    "        # Navigate to the h1 element containing the duration\n",
    "        h1_element = game_time_div.find_next(\"h1\")\n",
    "        if h1_element:\n",
    "            return h1_element.text.strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_game_duration(duration: str) -> int:\n",
    "    \"\"\"\n",
    "    Parse game duration string into total seconds.\n",
    "\n",
    "    Args:\n",
    "        duration: Game duration string (e.g., \"44:59\")\n",
    "\n",
    "    Returns:\n",
    "        Duration in seconds (e.g., 2699)\n",
    "    \"\"\"\n",
    "    total_seconds = 0\n",
    "\n",
    "    if duration:\n",
    "        time_parts = duration.split(\":\")\n",
    "        if len(time_parts) == 2:\n",
    "            minutes = int(time_parts[0])\n",
    "            seconds = int(time_parts[1])\n",
    "            total_seconds = minutes * 60 + seconds\n",
    "\n",
    "    return total_seconds\n",
    "\n",
    "\n",
    "def get_game_patch(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the game patch version from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object containing the parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        Game patch as a string (e.g., \"v15.4\") or None if not found\n",
    "    \"\"\"\n",
    "    # Find the div containing \"Game Time\" text\n",
    "    game_time_div = soup.find(string=\"Game Time\")\n",
    "\n",
    "    if game_time_div:\n",
    "        # Navigate to the parent elements to find the patch div\n",
    "        row_div = game_time_div.find_parent(\"div\").find_parent(\"div\").find_parent(\"div\")\n",
    "        if row_div:\n",
    "            # Find the div with class \"col-3 text-right\" which contains the patch\n",
    "            patch_div = row_div.find(\"div\", class_=\"col-3 text-right\")\n",
    "            if patch_div:\n",
    "                return patch_div.text.strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_game_version(patch: str) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Parse game patch version into major and minor components.\n",
    "\n",
    "    Args:\n",
    "        patch: Game patch version string (e.g., \"v15.4\")\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (major_patch, minor_patch)\n",
    "    \"\"\"\n",
    "    major_patch = 0\n",
    "    minor_patch = 0\n",
    "\n",
    "    if patch and patch.startswith(\"v\"):\n",
    "        version_parts = patch[1:].split(\".\")\n",
    "        if len(version_parts) >= 1:\n",
    "            major_patch = int(version_parts[0])\n",
    "        if len(version_parts) >= 2:\n",
    "            minor_patch = int(version_parts[1])\n",
    "\n",
    "    return major_patch, minor_patch\n",
    "\n",
    "\n",
    "patch = get_game_patch(soup)\n",
    "duration = get_game_duration(soup)\n",
    "\n",
    "print(patch, duration)\n",
    "\n",
    "print(parse_game_version(patch))\n",
    "print(parse_game_duration(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_info(soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract team information from the parsed HTML, including team names and winner.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object containing the parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - blue_team: Name of the blue team\n",
    "        - red_team: Name of the red team\n",
    "        - blue_won: 1 if blue team won, 0 if lost\n",
    "    \"\"\"\n",
    "    team_info = {\"blue_team\": None, \"red_team\": None, \"blue_won\": None}\n",
    "\n",
    "    # Find the blue team div\n",
    "    blue_div = soup.find(\"div\", class_=\"blue-line-header\")\n",
    "    if blue_div:\n",
    "        # Extract team name from the anchor tag\n",
    "        blue_team_anchor = blue_div.find(\"a\")\n",
    "        if blue_team_anchor:\n",
    "            team_info[\"blue_team\"] = blue_team_anchor.text.strip()\n",
    "\n",
    "        # Check if blue team won\n",
    "        blue_result = blue_div.text.strip()\n",
    "        team_info[\"blue_won\"] = 1 if \"- WIN\" in blue_result else 0\n",
    "\n",
    "    # Find the red team div\n",
    "    red_div = soup.find(\"div\", class_=\"red-line-header\")\n",
    "    if red_div:\n",
    "        # Extract team name from the anchor tag\n",
    "        red_team_anchor = red_div.find(\"a\")\n",
    "        if red_team_anchor:\n",
    "            team_info[\"red_team\"] = red_team_anchor.text.strip()\n",
    "\n",
    "    return team_info\n",
    "\n",
    "\n",
    "team_info = get_team_info(soup)\n",
    "\n",
    "print(team_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_champion_names(soup: BeautifulSoup) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract the champion names from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object containing the parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        List of 10 champion names in order: blue team (top to support) followed by red team (top to support)\n",
    "    \"\"\"\n",
    "    champion_names = []\n",
    "\n",
    "    # Find the tables with class \"playersInfosLine\"\n",
    "    player_tables = soup.find_all(\"table\", class_=\"playersInfosLine\")\n",
    "\n",
    "    # Process each table (blue team and red team)\n",
    "    for table in player_tables:\n",
    "        # Find all rows in the table (skip the header row)\n",
    "        rows = table.find_all(\"tr\")\n",
    "        # Skip the header row (first row)\n",
    "        for row in rows[1:]:  # Start from index 1 to skip header\n",
    "            # Find the champion image element\n",
    "            champion_img = row.find(\"img\", class_=\"champion_icon rounded-circle\")\n",
    "            if champion_img:\n",
    "                # Get the champion name from the alt attribute\n",
    "                champion_name = champion_img.get(\"alt\")\n",
    "                champion_names.append(champion_name)\n",
    "\n",
    "    return champion_names\n",
    "\n",
    "\n",
    "champion_names = get_champion_names(soup)\n",
    "\n",
    "print(champion_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict, Any\n",
    "from utils.rl.champions import Champion\n",
    "import difflib  # For fuzzy string matching\n",
    "\n",
    "\n",
    "def map_champion_names_to_ids(champion_names: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Map champion names to their corresponding IDs using the Champion enum with fuzzy matching.\n",
    "\n",
    "    Args:\n",
    "        champion_names: List of champion names\n",
    "\n",
    "    Returns:\n",
    "        List of champion IDs in the same order\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If a champion name cannot be mapped to an ID\n",
    "    \"\"\"\n",
    "    champion_ids = []\n",
    "\n",
    "    # Create a mapping of display names to champion IDs for quick lookup\n",
    "    name_to_id_map = {champion.display_name: champion.id for champion in Champion}\n",
    "    all_champion_names = list(name_to_id_map.keys())\n",
    "\n",
    "    # Create a reverse mapping for easier lookup (normalized name -> display name)\n",
    "    normalized_name_map = {}\n",
    "    for display_name in all_champion_names:\n",
    "        # Store both the lowercase version and a version with apostrophes removed\n",
    "        normalized_name_map[display_name.lower()] = display_name\n",
    "        normalized_name_map[display_name.lower().replace(\"'\", \"\")] = display_name\n",
    "\n",
    "    # Special case handling for known mismatches\n",
    "    special_cases = {\"nunu\": \"Nunu & Willump\"}\n",
    "\n",
    "    for name in champion_names:\n",
    "        champion_id = None\n",
    "\n",
    "        # Handle special cases\n",
    "        if name.lower() in special_cases:\n",
    "            name = special_cases[name.lower()]\n",
    "\n",
    "        # Try direct lookup first\n",
    "        if name in name_to_id_map:\n",
    "            champion_id = name_to_id_map[name]\n",
    "\n",
    "        # Try normalized lookup (handles case differences and missing apostrophes)\n",
    "        elif name.lower() in normalized_name_map:\n",
    "            display_name = normalized_name_map[name.lower()]\n",
    "            champion_id = name_to_id_map[display_name]\n",
    "\n",
    "        # Try without apostrophes\n",
    "        elif name.lower().replace(\"'\", \"\") in normalized_name_map:\n",
    "            display_name = normalized_name_map[name.lower().replace(\"'\", \"\")]\n",
    "            champion_id = name_to_id_map[display_name]\n",
    "\n",
    "        # If still not found, try fuzzy matching\n",
    "        if champion_id is None:\n",
    "            # Get the closest match using difflib\n",
    "            closest_matches = difflib.get_close_matches(\n",
    "                name, all_champion_names, n=1, cutoff=0.6\n",
    "            )\n",
    "\n",
    "            if closest_matches:\n",
    "                closest_match = closest_matches[0]\n",
    "                champion_id = name_to_id_map[closest_match]\n",
    "                print(\n",
    "                    f\"Warning: Using fuzzy match for '{name}' -> '{closest_match}' (ID: {champion_id})\"\n",
    "                )\n",
    "            else:\n",
    "                # If no match found, raise an error\n",
    "                raise ValueError(f\"Could not map champion name '{name}' to an ID.\")\n",
    "\n",
    "        champion_ids.append(champion_id)\n",
    "\n",
    "    return champion_ids\n",
    "\n",
    "\n",
    "champion_ids = map_champion_names_to_ids(champion_names)\n",
    "\n",
    "print(champion_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tournament_name(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract the tournament name from the parsed HTML.\n",
    "\n",
    "    Args:\n",
    "        soup: BeautifulSoup object containing the parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        Tournament name as a string or None if not found\n",
    "    \"\"\"\n",
    "    # Find the anchor tag with the specific href pattern\n",
    "    tournament_anchor = soup.find(\n",
    "        \"a\", href=lambda href: href and \"../tournament/tournament-stats\" in href\n",
    "    )\n",
    "\n",
    "    if tournament_anchor:\n",
    "        return tournament_anchor.text.strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "tournament_name = get_tournament_name(soup)\n",
    "print(tournament_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import random\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "\n",
    "def scrape_golgg_games(\n",
    "    start_game_id: int,\n",
    "    output_file_path: str,\n",
    "    min_major_version: int = 14,\n",
    "    request_delay: Tuple[float, float] = (1.0, 3.0),\n",
    "    limit: int = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape game data from gol.gg starting from a specific game ID and working backwards.\n",
    "    Save the results to a parquet file.\n",
    "\n",
    "    Args:\n",
    "        start_game_id: The game ID to start scraping from\n",
    "        output_file_path: Path where the parquet file will be saved\n",
    "        min_major_version: Minimum game major version to scrape (default: 14)\n",
    "        request_delay: Tuple of (min, max) seconds to delay between requests\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing all scraped game data\n",
    "    \"\"\"\n",
    "    # Check if the parquet file already exists and load it\n",
    "    if os.path.exists(output_file_path):\n",
    "        existing_df = pd.read_parquet(output_file_path)\n",
    "        print(f\"Loaded existing data with {len(existing_df)} games\")\n",
    "\n",
    "        # Get the set of game IDs that we already have\n",
    "        existing_game_ids = set(existing_df[\"golgg_id\"].tolist())\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"golgg_id\",\n",
    "                \"champion_ids\",\n",
    "                \"gameVersionMajorPatch\",\n",
    "                \"gameVersionMinorPatch\",\n",
    "                \"gameDuration\",\n",
    "                \"blueTeamName\",\n",
    "                \"redTeamName\",\n",
    "                \"tournamentName\",\n",
    "                \"team_100_win\",\n",
    "            ]\n",
    "        )\n",
    "        existing_game_ids = set()\n",
    "        print(\"No existing data found, creating new dataset\")\n",
    "\n",
    "    # Create a list to store new game data\n",
    "    new_games_data = []\n",
    "\n",
    "    # Initialize the current game ID\n",
    "    current_id = start_game_id\n",
    "\n",
    "    try:\n",
    "        # Main scraping loop\n",
    "        with tqdm(desc=\"Scraping games\") as pbar:\n",
    "            while current_id > 0:\n",
    "                # Skip if we already have this game ID\n",
    "                if current_id in existing_game_ids:\n",
    "                    print(f\"Game ID {current_id} already exists in dataset. Stopping.\")\n",
    "                    break\n",
    "\n",
    "                # Add random delay to avoid overloading the server\n",
    "                time.sleep(random.uniform(request_delay[0], request_delay[1]))\n",
    "\n",
    "                # Get game data\n",
    "                soup = get_golgg_game_stats(current_id)\n",
    "\n",
    "                # If game not found (404/302), skip to next ID\n",
    "                if soup is None:\n",
    "                    print(f\"Game ID {current_id} not found. Skipping.\")\n",
    "                    current_id -= 1\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Extract patch version\n",
    "                    patch = get_game_patch(soup)\n",
    "                    if patch:\n",
    "                        major_patch, minor_patch = parse_game_version(patch)\n",
    "\n",
    "                        # Check if we've reached a version below our minimum\n",
    "                        if major_patch < min_major_version:\n",
    "                            print(\n",
    "                                f\"Reached game with version {patch} below minimum {min_major_version}. Stopping.\"\n",
    "                            )\n",
    "                            break\n",
    "                    else:\n",
    "                        # If patch info not available, skip\n",
    "                        print(f\"No patch info for game ID {current_id}. Skipping.\")\n",
    "                        current_id -= 1\n",
    "                        continue\n",
    "\n",
    "                    # Extract other game data\n",
    "                    duration_str = get_game_duration(soup)\n",
    "                    duration_seconds = (\n",
    "                        parse_game_duration(duration_str) if duration_str else 0\n",
    "                    )\n",
    "\n",
    "                    team_info = get_team_info(soup)\n",
    "                    champion_names = get_champion_names(soup)\n",
    "\n",
    "                    # Map champion names to IDs\n",
    "                    try:\n",
    "                        champion_ids = map_champion_names_to_ids(champion_names)\n",
    "                    except ValueError as e:\n",
    "                        print(\n",
    "                            f\"Error mapping champion names for game ID {current_id}: {e}. Skipping.\"\n",
    "                        )\n",
    "                        current_id -= 1\n",
    "                        continue\n",
    "\n",
    "                    tournament_name = get_tournament_name(soup)\n",
    "\n",
    "                    # Create a record for this game\n",
    "                    game_data = {\n",
    "                        \"golgg_id\": current_id,\n",
    "                        \"champion_ids\": champion_ids,\n",
    "                        \"gameVersionMajorPatch\": major_patch,\n",
    "                        \"gameVersionMinorPatch\": minor_patch,\n",
    "                        \"gameDuration\": duration_seconds,\n",
    "                        \"blueTeamName\": team_info.get(\"blue_team\", \"\"),\n",
    "                        \"redTeamName\": team_info.get(\"red_team\", \"\"),\n",
    "                        \"tournamentName\": tournament_name or \"\",\n",
    "                        \"team_100_win\": team_info.get(\"blue_won\", None),\n",
    "                    }\n",
    "\n",
    "                    # Append to our list of new games\n",
    "                    new_games_data.append(game_data)\n",
    "\n",
    "                    # Every 100 games, save progress\n",
    "                    if len(new_games_data) % 100 == 0:\n",
    "                        # Combine existing data with new data\n",
    "                        combined_df = pd.concat(\n",
    "                            [existing_df, pd.DataFrame(new_games_data)],\n",
    "                            ignore_index=True,\n",
    "                        )\n",
    "\n",
    "                        # Save to parquet\n",
    "                        combined_df.to_parquet(output_file_path, index=False)\n",
    "                        print(f\"Saved progress: {len(combined_df)} total games\")\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix(\n",
    "                        {\n",
    "                            \"game_id\": current_id,\n",
    "                            \"version\": f\"v{major_patch}.{minor_patch}\",\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing game ID {current_id}: {e}. Skipping.\")\n",
    "\n",
    "                # Move to the previous game ID\n",
    "                current_id -= 1\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScraping interrupted by user\")\n",
    "\n",
    "    finally:\n",
    "        # Save final results if we have new data\n",
    "        if new_games_data:\n",
    "            # Combine existing data with new data\n",
    "            final_df = pd.concat(\n",
    "                [existing_df, pd.DataFrame(new_games_data)], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # Save to parquet\n",
    "            final_df.to_parquet(output_file_path, index=False)\n",
    "            print(f\"Final dataset saved with {len(final_df)} games\")\n",
    "            return final_df\n",
    "        else:\n",
    "            print(\"No new data collected\")\n",
    "            return existing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.match_prediction import RAW_PRO_GAMES_DIR\n",
    "\n",
    "# Define starting game ID (latest game from website)\n",
    "latest_game_id = 64545  # Replace with the latest game ID you find\n",
    "\n",
    "output_file_path = os.path.join(RAW_PRO_GAMES_DIR, \"pro_games.parquet\")\n",
    "\n",
    "# Run the scraper\n",
    "pro_games_df = scrape_golgg_games(\n",
    "    start_game_id=latest_game_id,\n",
    "    output_file_path=output_file_path,\n",
    "    min_major_version=14,\n",
    "    request_delay=(\n",
    "        1.0,\n",
    "        3.0,\n",
    "    ),  # Random delay between 1-3 seconds to be respectful to the server\n",
    ")\n",
    "\n",
    "# Display summary of collected data\n",
    "print(f\"Total games collected: {len(pro_games_df)}\")\n",
    "print(f\"Unique tournaments: {pro_games_df['tournamentName'].nunique()}\")\n",
    "print(f\"Games by major version:\")\n",
    "print(pro_games_df.groupby(\"gameVersionMajorPatch\").size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_games_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fix nunu imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "def extract_failed_nunu_ids(log_file_path):\n",
    "    \"\"\"\n",
    "    Extract game IDs that failed due to Nunu mapping issues from a log file.\n",
    "\n",
    "    Args:\n",
    "        log_file_path: Path to the log file\n",
    "\n",
    "    Returns:\n",
    "        List of game IDs that failed\n",
    "    \"\"\"\n",
    "    failed_ids = []\n",
    "\n",
    "    # Regular expression to match the error lines and extract the game ID\n",
    "    pattern = r\"Error mapping champion names for game ID (\\d+): Could not map champion name 'Nunu' to an ID\\.\"\n",
    "\n",
    "    with open(log_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                game_id = int(match.group(1))\n",
    "                failed_ids.append(game_id)\n",
    "\n",
    "    return failed_ids\n",
    "\n",
    "\n",
    "\n",
    "log_file_path = \"/Users/loyd/nunu.txt\"\n",
    "\n",
    "# Extract the failed IDs\n",
    "failed_ids = extract_failed_nunu_ids(log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(failed_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_specific_game_ids(\n",
    "    game_ids: List[int],\n",
    "    output_file_path: str,\n",
    "    request_delay: Tuple[float, float] = (1.0, 3.0),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add specific game IDs to the dataset that may have previously failed.\n",
    "\n",
    "    Args:\n",
    "        game_ids: List of game IDs to process and add\n",
    "        output_file_path: Path to the parquet file to update\n",
    "        request_delay: Tuple of (min, max) seconds to delay between requests\n",
    "\n",
    "    Returns:\n",
    "        Updated DataFrame containing all game data\n",
    "    \"\"\"\n",
    "    # Load existing data if it exists\n",
    "    if os.path.exists(output_file_path):\n",
    "        existing_df = pd.read_parquet(output_file_path)\n",
    "        print(f\"Loaded existing data with {len(existing_df)} games\")\n",
    "\n",
    "        # Get the set of game IDs that we already have\n",
    "        existing_game_ids = set(existing_df[\"golgg_id\"].tolist())\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(\n",
    "            columns=[\n",
    "                \"golgg_id\",\n",
    "                \"champion_ids\",\n",
    "                \"gameVersionMajorPatch\",\n",
    "                \"gameVersionMinorPatch\",\n",
    "                \"gameDuration\",\n",
    "                \"blueTeamName\",\n",
    "                \"redTeamName\",\n",
    "                \"tournamentName\",\n",
    "                \"team_100_win\",\n",
    "            ]\n",
    "        )\n",
    "        existing_game_ids = set()\n",
    "        print(\"No existing data found, creating new dataset\")\n",
    "\n",
    "    # Create a list to store new game data\n",
    "    new_games_data = []\n",
    "\n",
    "    # Filter out game IDs that already exist in the dataset\n",
    "    ids_to_process = [\n",
    "        game_id for game_id in game_ids if game_id not in existing_game_ids\n",
    "    ]\n",
    "\n",
    "    if not ids_to_process:\n",
    "        print(\"All specified game IDs already exist in the dataset. Nothing to add.\")\n",
    "        return existing_df\n",
    "\n",
    "    print(f\"Processing {len(ids_to_process)} new game IDs...\")\n",
    "\n",
    "    try:\n",
    "        # Process each game ID\n",
    "        with tqdm(total=len(ids_to_process), desc=\"Adding games\") as pbar:\n",
    "            for current_id in ids_to_process:\n",
    "                # Add random delay to avoid overloading the server\n",
    "                time.sleep(random.uniform(request_delay[0], request_delay[1]))\n",
    "\n",
    "                # Get game data\n",
    "                soup = get_golgg_game_stats(current_id)\n",
    "\n",
    "                # If game not found (404/302), skip to next ID\n",
    "                if soup is None:\n",
    "                    print(f\"Game ID {current_id} not found. Skipping.\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Extract patch version\n",
    "                    patch = get_game_patch(soup)\n",
    "                    if patch:\n",
    "                        major_patch, minor_patch = parse_game_version(patch)\n",
    "                    else:\n",
    "                        # If patch info not available, skip\n",
    "                        print(f\"No patch info for game ID {current_id}. Skipping.\")\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    # Extract other game data\n",
    "                    duration_str = get_game_duration(soup)\n",
    "                    duration_seconds = (\n",
    "                        parse_game_duration(duration_str) if duration_str else 0\n",
    "                    )\n",
    "\n",
    "                    team_info = get_team_info(soup)\n",
    "                    champion_names = get_champion_names(soup)\n",
    "\n",
    "                    # Map champion names to IDs with improved function\n",
    "                    try:\n",
    "                        champion_ids = map_champion_names_to_ids(champion_names)\n",
    "                    except ValueError as e:\n",
    "                        print(\n",
    "                            f\"Error mapping champion names for game ID {current_id}: {e}. Skipping.\"\n",
    "                        )\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    tournament_name = get_tournament_name(soup)\n",
    "\n",
    "                    # Create a record for this game\n",
    "                    game_data = {\n",
    "                        \"golgg_id\": current_id,\n",
    "                        \"champion_ids\": champion_ids,\n",
    "                        \"gameVersionMajorPatch\": major_patch,\n",
    "                        \"gameVersionMinorPatch\": minor_patch,\n",
    "                        \"gameDuration\": duration_seconds,\n",
    "                        \"blueTeamName\": team_info.get(\"blue_team\", \"\"),\n",
    "                        \"redTeamName\": team_info.get(\"red_team\", \"\"),\n",
    "                        \"tournamentName\": tournament_name or \"\",\n",
    "                        \"team_100_win\": team_info.get(\"blue_won\", None),\n",
    "                    }\n",
    "\n",
    "                    # Append to our list of new games\n",
    "                    new_games_data.append(game_data)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing game ID {current_id}: {e}. Skipping.\")\n",
    "                    pbar.update(1)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user\")\n",
    "\n",
    "    finally:\n",
    "        # Save final results if we have new data\n",
    "        if new_games_data:\n",
    "            # Combine existing data with new data\n",
    "            final_df = pd.concat(\n",
    "                [existing_df, pd.DataFrame(new_games_data)], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # Save to parquet\n",
    "            final_df.to_parquet(output_file_path, index=False)\n",
    "            print(f\"Final dataset saved with {len(final_df)} games\")\n",
    "            return final_df\n",
    "        else:\n",
    "            print(\"No new data collected\")\n",
    "            return existing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.match_prediction import RAW_PRO_GAMES_DIR\n",
    "import os\n",
    "\n",
    "# Path to your dataset\n",
    "output_file_path = os.path.join(RAW_PRO_GAMES_DIR, \"pro_games.parquet\")\n",
    "\n",
    "\n",
    "# Process the failed game IDs and add them to the dataset\n",
    "updated_df = add_specific_game_ids(\n",
    "    game_ids=failed_ids,\n",
    "    output_file_path=output_file_path,\n",
    "    request_delay=(1.0, 3.0),  # Random delay between 1-3 seconds\n",
    ")\n",
    "\n",
    "# Display summary of the updated dataset\n",
    "print(f\"Total games in updated dataset: {len(updated_df)}\")\n",
    "print(f\"Unique tournaments: {updated_df['tournamentName'].nunique()}\")\n",
    "print(f\"Games by major version:\")\n",
    "print(updated_df.groupby(\"gameVersionMajorPatch\").size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
