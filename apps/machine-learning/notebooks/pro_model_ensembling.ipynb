{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import DATA_DIR\n",
    "\n",
    "folder = DATA_DIR + \"/pro_ensembling\"\n",
    "\n",
    "solo_queue_csv = folder + \"/comp-games-with-predictions_solo_queue.csv\"\n",
    "clash_csv = folder + \"/comp-games-with-predictions_clash.csv\"\n",
    "\n",
    "solo_queue_df = pd.read_csv(solo_queue_csv)\n",
    "clash_df = pd.read_csv(clash_csv)\n",
    "\n",
    "solo_queue_df.head()\n",
    "\n",
    "clash_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solo_queue_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from typing import Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def evaluate_predictions(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate predictions using multiple metrics\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_prob),\n",
    "        \"log_loss\": log_loss(y_true, y_prob),\n",
    "    }\n",
    "\n",
    "\n",
    "# First, let's create a baseline using just pre-draft chances\n",
    "def evaluate_baseline(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate baseline performance using just pre-draft win chances\"\"\"\n",
    "    y_true = df[\"blue_team_won\"].values\n",
    "    y_prob = df[\"Blue Win Pre Draft\"].values\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "    return evaluate_predictions(y_true, y_pred, y_prob)\n",
    "\n",
    "\n",
    "# Prepare features for the full model\n",
    "def prepare_features(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df[\"prediction_diff\"] = df[\"solo_queue_prediction\"] - df[\"clash_prediction\"]\n",
    "\n",
    "    features = [\n",
    "        \"solo_queue_prediction\",\n",
    "        \"clash_prediction\",\n",
    "        \"prediction_diff\",\n",
    "        \"Blue Win Pre Draft\",\n",
    "    ]\n",
    "\n",
    "    X = df[features].values  # Shape: (n_samples, 4)\n",
    "    y = df[\"blue_team_won\"].values  # Shape: (n_samples,)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "combined_df = pd.merge(\n",
    "    solo_queue_df.rename(\n",
    "        columns={\n",
    "            \"model_prediction\": \"solo_queue_prediction\",\n",
    "            \"model_confidence\": \"solo_queue_confidence\",\n",
    "        }\n",
    "    ),\n",
    "    clash_df[[\"id\", \"model_prediction\"]].rename(\n",
    "        columns={\"model_prediction\": \"clash_prediction\"}\n",
    "    ),\n",
    "    on=\"id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# Filter for close games\n",
    "close_games_df = combined_df[\n",
    "    (combined_df['Blue Win Pre Draft'] >= 0.35) & \n",
    "    (combined_df['Blue Win Pre Draft'] <= 0.65)\n",
    "]\n",
    "\n",
    "print(f\"Total games: {len(combined_df)}\")\n",
    "print(f\"Close games: {len(close_games_df)} ({len(close_games_df)/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# Print baseline metrics\n",
    "print(\"Baseline (Pre-draft only) Metrics:\")\n",
    "baseline_metrics = evaluate_baseline(combined_df)\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Prepare full model data\n",
    "X, y = prepare_features(combined_df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Train XGBoost model with early stopping\n",
    "model = XGBClassifier(\n",
    "    n_estimators=1000,  # More trees, will use early stopping\n",
    "    learning_rate=0.01,  # Lower learning rate to prevent overfitting\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    eval_metric=['logloss', 'error'],  # Move eval_metric here\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "# Train with evaluation sets\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=eval_set,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "train_preds = model.predict(X_train)\n",
    "train_probs = model.predict_proba(X_train)[:, 1]\n",
    "test_preds = model.predict(X_test)\n",
    "test_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate full model\n",
    "print(\"\\nFull Model Metrics:\")\n",
    "print(\"Training Set:\")\n",
    "train_metrics = evaluate_predictions(y_train, train_preds, train_probs)\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "test_metrics = evaluate_predictions(y_test, test_preds, test_probs)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "results = model.evals_result()\n",
    "epochs = len(results[\"validation_0\"][\"error\"])\n",
    "x_axis = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_axis, results[\"validation_0\"][\"error\"], label=\"Train\")\n",
    "plt.plot(x_axis, results[\"validation_1\"][\"error\"], label=\"Test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Classification Error\")\n",
    "plt.title(\"Model Error vs Epoch\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_axis, results[\"validation_0\"][\"logloss\"], label=\"Train\")\n",
    "plt.plot(x_axis, results[\"validation_1\"][\"logloss\"], label=\"Test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"Log Loss vs Epoch\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "feature_names = [\n",
    "    \"Solo Queue Prediction\",\n",
    "    \"Clash Prediction\",\n",
    "    \"Prediction Difference\",\n",
    "    \"Pre-Draft Win Chance\",\n",
    "]\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "importance_dict = dict(zip(feature_names, model.feature_importances_))\n",
    "for feature, importance in sorted(\n",
    "    importance_dict.items(), key=lambda x: x[1], reverse=True\n",
    "):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "def evaluate_predictions(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate predictions using multiple metrics\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_prob),\n",
    "        \"log_loss\": log_loss(y_true, y_prob),\n",
    "    }\n",
    "\n",
    "\n",
    "# Filter for close games\n",
    "combined_df = pd.merge(\n",
    "    solo_queue_df.rename(\n",
    "        columns={\n",
    "            \"model_prediction\": \"solo_queue_prediction\",\n",
    "            \"model_confidence\": \"solo_queue_confidence\",\n",
    "        }\n",
    "    ),\n",
    "    clash_df[[\"id\", \"model_prediction\"]].rename(\n",
    "        columns={\"model_prediction\": \"clash_prediction\"}\n",
    "    ),\n",
    "    on=\"id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# Filter for close games\n",
    "close_games_df = combined_df[\n",
    "    (combined_df[\"Blue Win Pre Draft\"] >= 0.40)\n",
    "    & (combined_df[\"Blue Win Pre Draft\"] <= 0.60)\n",
    "]\n",
    "\n",
    "print(f\"Total games: {len(combined_df)}\")\n",
    "print(\n",
    "    f\"Close games: {len(close_games_df)} ({len(close_games_df)/len(combined_df)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# First, evaluate baseline on close games\n",
    "print(\"\\nBaseline (Pre-draft only) Metrics on Close Games:\")\n",
    "baseline_metrics = evaluate_baseline(close_games_df)\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "# Prepare features for the full model\n",
    "def prepare_features(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df[\"prediction_diff\"] = df[\"solo_queue_prediction\"] - df[\"clash_prediction\"]\n",
    "\n",
    "    features = [\n",
    "        \"solo_queue_prediction\",\n",
    "        \"clash_prediction\",\n",
    "        \"prediction_diff\",\n",
    "        \"Blue Win Pre Draft\",\n",
    "    ]\n",
    "\n",
    "    X = df[features].values\n",
    "    y = df[\"blue_team_won\"].values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Prepare full model data\n",
    "X, y = prepare_features(close_games_df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Train XGBoost model\n",
    "model = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.001,\n",
    "    max_depth=2,\n",
    "    random_state=42,\n",
    "    eval_metric=[\"logloss\", \"error\"],\n",
    ")\n",
    "\n",
    "# Train with evaluation sets\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "model.fit(X_train, y_train, eval_set=eval_set, verbose=True)\n",
    "\n",
    "# Get predictions\n",
    "train_preds = model.predict(X_train)\n",
    "train_probs = model.predict_proba(X_train)[:, 1]\n",
    "test_preds = model.predict(X_test)\n",
    "test_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFull Model Metrics on Close Games:\")\n",
    "print(\"Training Set:\")\n",
    "train_metrics = evaluate_predictions(y_train, train_preds, train_probs)\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "test_metrics = evaluate_predictions(y_test, test_preds, test_probs)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_names = [\n",
    "    \"Solo Queue Prediction\",\n",
    "    \"Clash Prediction\",\n",
    "    \"Prediction Difference\",\n",
    "    \"Pre-Draft Win Chance\",\n",
    "]\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "importance_dict = dict(zip(feature_names, model.feature_importances_))\n",
    "for feature, importance in sorted(\n",
    "    importance_dict.items(), key=lambda x: x[1], reverse=True\n",
    "):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Additional analysis of prediction improvements\n",
    "print(\"\\nDetailed Analysis of Close Games:\")\n",
    "print(\n",
    "    f\"Average pre-draft win chance: {close_games_df['Blue Win Pre Draft'].mean():.4f}\"\n",
    ")\n",
    "print(f\"Actual blue side win rate: {close_games_df['blue_team_won'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def prepare_features_for_error_analysis(df: pd.DataFrame) -> Tuple[np.ndarray, Dict[str, LabelEncoder]]:\n",
    "    \"\"\"Prepare features including champions and team names\"\"\"\n",
    "    # Create label encoders for categorical variables\n",
    "    categorical_columns = [\n",
    "        'region_name', 'blue_team_name', 'red_team_name',\n",
    "        'blue_top_name', 'blue_jungle_name', 'blue_mid_name', \n",
    "        'blue_bot_name', 'blue_support_name',\n",
    "        'red_top_name', 'red_jungle_name', 'red_mid_name', \n",
    "        'red_bot_name', 'red_support_name'\n",
    "    ]\n",
    "    \n",
    "    encoders = {}\n",
    "    encoded_df = df.copy()\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    for col in categorical_columns:\n",
    "        encoders[col] = LabelEncoder()\n",
    "        encoded_df[col] = encoders[col].fit_transform(df[col])\n",
    "    \n",
    "    # Select features for the model\n",
    "    features = [\n",
    "        'Blue Win Pre Draft', 'Red Win Pre Draft',\n",
    "        *categorical_columns\n",
    "    ]\n",
    "    \n",
    "    return encoded_df[features].values, encoders\n",
    "\n",
    "# Prepare the data\n",
    "X, encoders = prepare_features_for_error_analysis(solo_queue_df)\n",
    "\n",
    "# Create two target variables\n",
    "y_error = solo_queue_df['model_error'].values\n",
    "y_confidence = solo_queue_df['model_confidence'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_error_train, y_error_test, y_conf_train, y_conf_test = train_test_split(\n",
    "    X, y_error, y_confidence, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train models for both error and confidence\n",
    "error_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "confidence_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit models\n",
    "error_model.fit(X_train, y_error_train)\n",
    "confidence_model.fit(X_train, y_conf_train)\n",
    "\n",
    "# Get predictions\n",
    "error_preds = error_model.predict(X_test)\n",
    "conf_preds = confidence_model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Model Error Prediction Performance:\")\n",
    "print(f\"MSE: {mean_squared_error(y_error_test, error_preds):.4f}\")\n",
    "print(f\"R2 Score: {r2_score(y_error_test, error_preds):.4f}\")\n",
    "\n",
    "print(\"\\nModel Confidence Prediction Performance:\")\n",
    "print(f\"MSE: {mean_squared_error(y_conf_test, conf_preds):.4f}\")\n",
    "print(f\"R2 Score: {r2_score(y_conf_test, conf_preds):.4f}\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = [\n",
    "    'Blue Win Pre Draft', 'Red Win Pre Draft',\n",
    "    'Region', 'Blue Team', 'Red Team',\n",
    "    'Blue Top', 'Blue Jungle', 'Blue Mid', 'Blue Bot', 'Blue Support',\n",
    "    'Red Top', 'Red Jungle', 'Red Mid', 'Red Bot', 'Red Support'\n",
    "]\n",
    "\n",
    "# Print feature importances for both models\n",
    "print(\"\\nTop 10 Features Contributing to Model Error:\")\n",
    "error_importance = dict(zip(feature_names, error_model.feature_importances_))\n",
    "for feature, importance in sorted(error_importance.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 Features Contributing to Model Confidence:\")\n",
    "conf_importance = dict(zip(feature_names, confidence_model.feature_importances_))\n",
    "for feature, importance in sorted(conf_importance.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Additional analysis: Find patterns in high-error games\n",
    "high_error_threshold = np.percentile(solo_queue_df['model_error'], 75)\n",
    "high_error_games = solo_queue_df[solo_queue_df['model_error'] > high_error_threshold]\n",
    "\n",
    "print(\"\\nAnalysis of High-Error Games:\")\n",
    "print(f\"Number of high-error games: {len(high_error_games)}\")\n",
    "print(\"\\nMost common teams in high-error games:\")\n",
    "print(\"Blue side:\")\n",
    "print(high_error_games['blue_team_name'].value_counts().head())\n",
    "print(\"\\nRed side:\")\n",
    "print(high_error_games['red_team_name'].value_counts().head())\n",
    "\n",
    "print(\"\\nMost common champions in high-error games:\")\n",
    "for role in ['top', 'jungle', 'mid', 'bot', 'support']:\n",
    "    print(f\"\\n{role.capitalize()} lane:\")\n",
    "    blue_champs = high_error_games[f'blue_{role}_name'].value_counts().head(3)\n",
    "    red_champs = high_error_games[f'red_{role}_name'].value_counts().head(3)\n",
    "    print(\"Blue side:\", dict(blue_champs))\n",
    "    print(\"Red side:\", dict(red_champs))\n",
    "\n",
    "# Calculate average error by region\n",
    "print(\"\\nAverage model error by region:\")\n",
    "region_errors = solo_queue_df.groupby('region_name')['model_error'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "print(region_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
