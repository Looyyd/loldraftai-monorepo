{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DraftGap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook used to create this blog post:\n",
    "https://loldraftai.com/blog/draftgap-vs-loldraftai-comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Endpoint Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the API endpoint\n",
    "API_URL = \"http://localhost:3000/analyze\"\n",
    "\n",
    "# Check if the server is running\n",
    "try:\n",
    "    health_response = requests.get(\"http://localhost:3000/health\")\n",
    "    health_data = health_response.json()\n",
    "    print(f\"Server status: {health_data['status']}\")\n",
    "    print(f\"Datasets loaded: {health_data['datasetsLoaded']}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\n",
    "        \"⚠️ API server not running! Please start the server with 'npm start' in the draftgap-api directory.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CSV with 1000 rows comparing both models\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load label encoders to decode champion IDs\n",
    "ENCODERS_PATH = os.path.join('/Users/loyd/draftking/apps/machine-learning/data', \"label_encoders.pkl\")\n",
    "with open(ENCODERS_PATH, \"rb\") as f:\n",
    "    label_encoders = pickle.load(f)\n",
    "\n",
    "# Function to fetch DraftGap prediction\n",
    "def fetch_draftgap_prediction(champion_ids):\n",
    "    # Split the champion IDs into ally (blue) and enemy (red) teams\n",
    "    ally_team = {\n",
    "        \"0\": int(champion_ids[0]),  # Top\n",
    "        \"1\": int(champion_ids[1]),  # Jungle\n",
    "        \"2\": int(champion_ids[2]),  # Mid\n",
    "        \"3\": int(champion_ids[3]),  # ADC\n",
    "        \"4\": int(champion_ids[4]),  # Support\n",
    "    }\n",
    "    \n",
    "    enemy_team = {\n",
    "        \"0\": int(champion_ids[5]),  # Top\n",
    "        \"1\": int(champion_ids[6]),  # Jungle\n",
    "        \"2\": int(champion_ids[7]),  # Mid\n",
    "        \"3\": int(champion_ids[8]),  # ADC\n",
    "        \"4\": int(champion_ids[9]),  # Support\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"allyTeam\": ally_team,\n",
    "        \"enemyTeam\": enemy_team,\n",
    "        \"riskLevel\": \"medium\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\"http://localhost:3000/analyze\", json=payload)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['winrate']\n",
    "        else:\n",
    "            print(f\"Error from DraftGap API: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to fetch LoLDraftAI prediction\n",
    "def fetch_loldraftai_prediction(champion_ids):\n",
    "    input_data = {\n",
    "        \"champion_ids\": champion_ids.tolist(),\n",
    "        \"numerical_elo\": 3,  # Emerald\n",
    "        \"patch\": \"15.04\",\n",
    "    }\n",
    "    \n",
    "    headers = {\"X-API-Key\": \"example_token\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:8000/predict\", json=input_data, headers=headers\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"win_probability\"]\n",
    "        else:\n",
    "            print(f\"Error from LoLDraftAI API: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load and filter test data\n",
    "def load_filtered_test_data(max_rows=1000):\n",
    "    all_files = sorted(glob.glob(\"/Users/loyd/draftking/apps/machine-learning/data/prepared_data/test/test_*.parquet\"))\n",
    "    \n",
    "    filtered_data = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        if total_rows >= max_rows:\n",
    "            break\n",
    "            \n",
    "        df = pd.read_parquet(file_path)\n",
    "        # Since there are no games with version 15.4, let's use version 15.3 instead\n",
    "        filtered_df = df[(df['gameVersionMajorPatch'] == 15) & (df['gameVersionMinorPatch'] == 3)]\n",
    "        \n",
    "        if len(filtered_df) > 0:\n",
    "            # Take only what we need to reach max_rows\n",
    "            rows_needed = min(max_rows - total_rows, len(filtered_df))\n",
    "            filtered_data.append(filtered_df.head(rows_needed))\n",
    "            total_rows += rows_needed\n",
    "            print(f\"Added {rows_needed} rows from {file_path}\")\n",
    "            \n",
    "    if total_rows < max_rows:\n",
    "        print(f\"Could only find {total_rows} rows with version 15.3\")\n",
    "        \n",
    "    # Combine all filtered data\n",
    "    if filtered_data:\n",
    "        return pd.concat(filtered_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Main processing function\n",
    "def create_validation_predictions_csv():\n",
    "    # Load and filter data to get 1000 rows\n",
    "    df = load_filtered_test_data(max_rows=5000)  # Using 100 for a quick demonstration\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"No data found matching criteria\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Processing {len(df)} matches from version 15.3\")\n",
    "    \n",
    "    # Initialize the results dataframe\n",
    "    results = []\n",
    "    \n",
    "    # For each row in the filtered dataframe\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing matches\"):\n",
    "        # Get the encoded champion IDs and convert to original IDs\n",
    "        encoded_champion_ids = row['champion_ids']\n",
    "        original_champion_ids = label_encoders['champion_ids'].inverse_transform(encoded_champion_ids)\n",
    "        \n",
    "        # Get match ID\n",
    "        match_id = row['matchId']\n",
    "        \n",
    "        # Get actual result (True = blue win, False = red win)\n",
    "        actual_result = int(row['win_prediction'])\n",
    "        \n",
    "        # Get predictions from both models\n",
    "        draftgap_prediction = fetch_draftgap_prediction(original_champion_ids)\n",
    "        loldraftai_prediction = fetch_loldraftai_prediction(np.array(original_champion_ids))\n",
    "        \n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'matchId': match_id,\n",
    "            'draftgap_prediction': draftgap_prediction if draftgap_prediction is not None else float('nan'),\n",
    "            'loldraftai_prediction': loldraftai_prediction if loldraftai_prediction is not None else float('nan'),\n",
    "            'actual_result': actual_result,\n",
    "            'champion_ids': original_champion_ids.tolist()  # Store the original champion IDs\n",
    "        })\n",
    "    \n",
    "    # Create dataframe from results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save as CSV\n",
    "    results_df.to_csv('validation_predictions.csv', index=False)\n",
    "    \n",
    "    print(f\"Saved {len(results_df)} predictions to validation_predictions.csv\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run the main function to generate the results\n",
    "results_df = create_validation_predictions_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results - focusing only on accuracy and model comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Function to analyze and compare models\n",
    "def analyze_model_comparison(df):\n",
    "    # Make sure we only use rows where both predictions are available\n",
    "    valid_df = df.dropna()\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        print(\"No valid predictions to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Convert predictions to binary outcomes (>0.5 = blue win)\n",
    "    draftgap_binary = (valid_df['draftgap_prediction'] > 0.5).astype(int)\n",
    "    loldraftai_binary = (valid_df['loldraftai_prediction'] > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate accuracy for each model\n",
    "    draftgap_accuracy = accuracy_score(valid_df['actual_result'], draftgap_binary)\n",
    "    loldraftai_accuracy = accuracy_score(valid_df['actual_result'], loldraftai_binary)\n",
    "    \n",
    "    print(f\"Total valid samples: {len(valid_df)}\")\n",
    "    print(\"\\nOverall Accuracy:\")\n",
    "    print(f\"- DraftGap: {draftgap_accuracy:.4f} ({int(draftgap_accuracy * len(valid_df))}/{len(valid_df)} correct)\")\n",
    "    print(f\"- LoLDraftAI: {loldraftai_accuracy:.4f} ({int(loldraftai_accuracy * len(valid_df))}/{len(valid_df)} correct)\")\n",
    "    \n",
    "    # Check when models agree vs disagree\n",
    "    agree_mask = draftgap_binary == loldraftai_binary\n",
    "    agree_count = agree_mask.sum()\n",
    "    disagree_count = len(valid_df) - agree_count\n",
    "    \n",
    "    print(f\"\\nModel Agreement: {agree_count}/{len(valid_df)} samples ({agree_count/len(valid_df):.2%})\")\n",
    "    print(f\"Model Disagreement: {disagree_count}/{len(valid_df)} samples ({disagree_count/len(valid_df):.2%})\")\n",
    "    \n",
    "    # Compute accuracy when models agree\n",
    "    if agree_count > 0:\n",
    "        agree_df = valid_df[agree_mask]\n",
    "        agree_accurate = (draftgap_binary[agree_mask] == agree_df['actual_result']).mean()\n",
    "        print(f\"Accuracy when models agree: {agree_accurate:.4f}\")\n",
    "    \n",
    "    # Compute accuracy when models disagree\n",
    "    if disagree_count > 0:\n",
    "        disagree_df = valid_df[~agree_mask]\n",
    "        draftgap_disagree_acc = accuracy_score(disagree_df['actual_result'], draftgap_binary[~agree_mask])\n",
    "        loldraftai_disagree_acc = accuracy_score(disagree_df['actual_result'], loldraftai_binary[~agree_mask])\n",
    "        print(f\"When models disagree, DraftGap accuracy: {draftgap_disagree_acc:.4f}\")\n",
    "        print(f\"When models disagree, LoLDraftAI accuracy: {loldraftai_disagree_acc:.4f}\")\n",
    "    \n",
    "    # Create confusion matrices\n",
    "    draftgap_cm = confusion_matrix(valid_df['actual_result'], draftgap_binary)\n",
    "    loldraftai_cm = confusion_matrix(valid_df['actual_result'], loldraftai_binary)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    sns.heatmap(draftgap_cm, annot=True, fmt='d', cmap='Blues', \n",
    "               xticklabels=['Red Win', 'Blue Win'], yticklabels=['Red Win', 'Blue Win'], ax=axes[0])\n",
    "    axes[0].set_title('DraftGap Confusion Matrix')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    \n",
    "    sns.heatmap(loldraftai_cm, annot=True, fmt='d', cmap='Blues', \n",
    "               xticklabels=['Red Win', 'Blue Win'], yticklabels=['Red Win', 'Blue Win'], ax=axes[1])\n",
    "    axes[1].set_title('LoLDraftAI Confusion Matrix')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create direct comparison scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # We'll color points by their actual outcome\n",
    "    scatter = plt.scatter(valid_df['draftgap_prediction'], \n",
    "                         valid_df['loldraftai_prediction'], \n",
    "                         c=valid_df['actual_result'], \n",
    "                         cmap='coolwarm', \n",
    "                         alpha=0.7, \n",
    "                         s=50)\n",
    "    \n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--')\n",
    "    plt.axvline(x=0.5, color='gray', linestyle='--')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel('DraftGap Prediction (Blue Win Probability)')\n",
    "    plt.ylabel('LoLDraftAI Prediction (Blue Win Probability)')\n",
    "    plt.title('Comparison of Model Predictions')\n",
    "    plt.colorbar(scatter, label='Actual Outcome (0=Red Win, 1=Blue Win)')\n",
    "    \n",
    "    # Add regions\n",
    "    plt.text(0.25, 0.75, \"DraftGap: Red Win\\nLoLDraftAI: Blue Win\", \n",
    "             horizontalalignment='center', size=10, alpha=0.7)\n",
    "    plt.text(0.75, 0.25, \"DraftGap: Blue Win\\nLoLDraftAI: Red Win\", \n",
    "             horizontalalignment='center', size=10, alpha=0.7)\n",
    "    plt.text(0.25, 0.25, \"Both Predict\\nRed Win\", \n",
    "             horizontalalignment='center', size=10, alpha=0.7)\n",
    "    plt.text(0.75, 0.75, \"Both Predict\\nBlue Win\", \n",
    "             horizontalalignment='center', size=10, alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for further use\n",
    "    return {\n",
    "        'draftgap_accuracy': draftgap_accuracy,\n",
    "        'loldraftai_accuracy': loldraftai_accuracy,\n",
    "        'agreement_rate': agree_count/len(valid_df),\n",
    "        'draftgap_cm': draftgap_cm,\n",
    "        'loldraftai_cm': loldraftai_cm\n",
    "    }\n",
    "\n",
    "# If results_df exists, analyze it\n",
    "if 'results_df' in locals() and results_df is not None and len(results_df) > 0:\n",
    "    metrics = analyze_model_comparison(results_df)\n",
    "else:\n",
    "    print(\"No results available - run the data generation cell first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of model comparison with detailed champion information\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.rl.champions import Champion\n",
    "\n",
    "# If metrics exists, create a summary\n",
    "if 'metrics' in locals() and metrics is not None and 'results_df' in locals() and results_df is not None:\n",
    "    # Calculate absolute error for each prediction\n",
    "    valid_df = results_df.dropna().copy()\n",
    "    \n",
    "    valid_df['draftgap_error'] = np.abs(valid_df['draftgap_prediction'] - valid_df['actual_result'])\n",
    "    valid_df['loldraftai_error'] = np.abs(valid_df['loldraftai_prediction'] - valid_df['actual_result'])\n",
    "    \n",
    "    # Determine which model has closer prediction for each row\n",
    "    valid_df['better_model'] = np.where(\n",
    "        valid_df['draftgap_error'] < valid_df['loldraftai_error'],\n",
    "        'DraftGap',\n",
    "        np.where(\n",
    "            valid_df['loldraftai_error'] < valid_df['draftgap_error'],\n",
    "            'LoLDraftAI',\n",
    "            'Tie'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Calculate mean absolute error\n",
    "    draftgap_mae = valid_df['draftgap_error'].mean()\n",
    "    loldraftai_mae = valid_df['loldraftai_error'].mean()\n",
    "    \n",
    "    # Count how many times each model is better\n",
    "    model_comparison = valid_df['better_model'].value_counts()\n",
    "    \n",
    "    print(\"# Model Comparison Summary\\n\")\n",
    "    \n",
    "    # Print overall accuracy comparison\n",
    "    print(\"## Overall Accuracy\")\n",
    "    if metrics['draftgap_accuracy'] > metrics['loldraftai_accuracy']:\n",
    "        acc_diff = metrics['draftgap_accuracy'] - metrics['loldraftai_accuracy']\n",
    "        winner = \"DraftGap\"\n",
    "    elif metrics['loldraftai_accuracy'] > metrics['draftgap_accuracy']:\n",
    "        acc_diff = metrics['loldraftai_accuracy'] - metrics['draftgap_accuracy']\n",
    "        winner = \"LoLDraftAI\"\n",
    "    else:\n",
    "        acc_diff = 0\n",
    "        winner = \"Tie\"\n",
    "        \n",
    "    print(f\"Winner: {winner}\" + (f\" (+{acc_diff:.4f})\" if acc_diff > 0 else \"\"))\n",
    "    \n",
    "    # Print error comparison\n",
    "    print(\"\\n## Mean Absolute Error\")\n",
    "    if draftgap_mae < loldraftai_mae:\n",
    "        error_diff = loldraftai_mae - draftgap_mae\n",
    "        error_winner = \"DraftGap\"\n",
    "    elif loldraftai_mae < draftgap_mae:\n",
    "        error_diff = draftgap_mae - loldraftai_mae\n",
    "        error_winner = \"LoLDraftAI\"\n",
    "    else:\n",
    "        error_diff = 0\n",
    "        error_winner = \"Tie\"\n",
    "        \n",
    "    print(f\"DraftGap MAE: {draftgap_mae:.4f}\")\n",
    "    print(f\"LoLDraftAI MAE: {loldraftai_mae:.4f}\")\n",
    "    print(f\"Winner: {error_winner}\" + (f\" (-{error_diff:.4f})\" if error_diff > 0 else \"\"))\n",
    "    \n",
    "    # Print how often each model is closer to the actual result\n",
    "    print(\"\\n## Row-by-Row Comparison\")\n",
    "    for model, count in model_comparison.items():\n",
    "        percentage = count / len(valid_df) * 100\n",
    "        print(f\"{model}: {count} rows ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Create a map from champion ID to name\n",
    "    champion_id_to_name = {c.id: c.display_name for c in Champion}\n",
    "    \n",
    "    # OPTIMIZATION: The champion IDs are already in create_validation_predictions_csv()\n",
    "    # We don't need to reload the whole dataset for each match, just append champion data\n",
    "    \n",
    "    # First, modify our results_df to include champion IDs\n",
    "    # We need to merge it with the detailed data structure\n",
    "    detailed_rows = []\n",
    "    \n",
    "    for idx, row in valid_df.iterrows():\n",
    "        match_id = row['matchId']\n",
    "        \n",
    "        # Create a dictionary for this row with all the comparison data\n",
    "        detailed_row = {\n",
    "            'matchId': match_id,\n",
    "            'draftgap_prediction': row['draftgap_prediction'],\n",
    "            'loldraftai_prediction': row['loldraftai_prediction'],\n",
    "            'actual_result': row['actual_result'],\n",
    "            'draftgap_error': row['draftgap_error'],\n",
    "            'loldraftai_error': row['loldraftai_error'],\n",
    "            'better_model': row['better_model']\n",
    "        }\n",
    "        \n",
    "        # Add champion information directly from the results_df\n",
    "        # The champion_ids should be stored in the original results_df from create_validation_predictions_csv\n",
    "        if 'champion_ids' in row:\n",
    "            original_champion_ids = row['champion_ids']\n",
    "        else:\n",
    "            # If not available, use dummy values\n",
    "            original_champion_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            \n",
    "        detailed_row['champion_ids'] = original_champion_ids\n",
    "        \n",
    "        # Add position-specific champion names\n",
    "        positions = ['Top', 'Jungle', 'Mid', 'ADC', 'Support']\n",
    "        \n",
    "        for i, position in enumerate(positions):\n",
    "            # Blue team (positions 0-4)\n",
    "            champ_id = int(original_champion_ids[i]) if i < len(original_champion_ids) else 0\n",
    "            detailed_row[f'Blue_{position}'] = champion_id_to_name.get(champ_id, f\"Unknown ({champ_id})\")\n",
    "            \n",
    "            # Red team (positions 5-9)\n",
    "            champ_id = int(original_champion_ids[i+5]) if i+5 < len(original_champion_ids) else 0\n",
    "            detailed_row[f'Red_{position}'] = champion_id_to_name.get(champ_id, f\"Unknown ({champ_id})\")\n",
    "        \n",
    "        detailed_rows.append(detailed_row)\n",
    "    \n",
    "    # Create the detailed dataframe\n",
    "    comparison_df = pd.DataFrame(detailed_rows)\n",
    "    \n",
    "    # Save the enhanced comparison CSV\n",
    "    comparison_df.to_csv('model_comparison_details.csv', index=False)\n",
    "    print(f\"\\nEnhanced detailed comparison saved to model_comparison_details.csv\")\n",
    "    print(f\"Added champion IDs and names for each position for easier human readability\")\n",
    "    \n",
    "else:\n",
    "    print(\"No metrics available - run the analysis cell first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's preview the enhanced comparison CSV\n",
    "if 'comparison_df' in locals() and comparison_df is not None and len(comparison_df) > 0:\n",
    "    # Display a sample of the enhanced comparison data\n",
    "    print(\"Sample of the enhanced model comparison details:\")\n",
    "    display_columns = [\n",
    "        'matchId', 'actual_result', \n",
    "        'draftgap_prediction', 'loldraftai_prediction', \n",
    "        'better_model',\n",
    "        'Blue_Top', 'Blue_Jungle', 'Blue_Mid', 'Blue_ADC', 'Blue_Support',\n",
    "        'Red_Top', 'Red_Jungle', 'Red_Mid', 'Red_ADC', 'Red_Support'\n",
    "    ]\n",
    "    \n",
    "    # Display the first few rows with just the key columns\n",
    "    print(comparison_df[display_columns].head())\n",
    "    \n",
    "    # Show the column names in the CSV\n",
    "    print(\"\\nAll columns in the model_comparison_details.csv:\")\n",
    "    print(\", \".join(comparison_df.columns))\n",
    "    \n",
    "    # Show some statistics\n",
    "    print(f\"\\nTotal matches analyzed: {len(comparison_df)}\")\n",
    "    print(f\"DraftGap better: {len(comparison_df[comparison_df['better_model'] == 'DraftGap'])} matches\")\n",
    "    print(f\"LoLDraftAI better: {len(comparison_df[comparison_df['better_model'] == 'LoLDraftAI'])} matches\")\n",
    "    print(f\"Tie: {len(comparison_df[comparison_df['better_model'] == 'Tie'])} matches\")\n",
    "else:\n",
    "    print(\"No comparison data available - run the analysis cells first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
