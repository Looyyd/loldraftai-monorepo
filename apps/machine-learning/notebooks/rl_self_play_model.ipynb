{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from IPython.display import display, Image as IPImage\n",
    "import os\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from utils import DATA_DIR\n",
    "from utils.rl.env import FlexibleRoleDraftEnv, action_mask_fn\n",
    "from utils.rl.self_play import ModelPool, SelfPlayWithPoolWrapper\n",
    "from utils.rl.visualizer import integrate_with_env\n",
    "from utils.match_prediction import get_best_device\n",
    "from utils.rl import ROLE_CHAMPIONS_PATH\n",
    "\n",
    "import warnings\n",
    "\n",
    "# sb3_contrib is not updated to latest api, this is the message we are ignoring:\n",
    "# WARN: env.get_action_mask to get variables from other wrappers is deprecated and will be removed in v1.0\n",
    "warnings.filterwarnings(\"ignore\", message=\".*env.get_action_mask.*\")\n",
    "\n",
    "device = get_best_device()\n",
    "\n",
    "role_rates_path = os.path.join(\n",
    "    os.path.dirname(ROLE_CHAMPIONS_PATH), \"champion_role_rates.json\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a simplified model pool for visualization\n",
    "class VisualizationModelPool(ModelPool):\n",
    "    def __init__(self, model_path: str):\n",
    "        super().__init__(save_dir=\"\")  # We don't need save_dir for visualization\n",
    "        self.model = MaskablePPO.load(\n",
    "            model_path, device=device\n",
    "        )  # need to specify device to avoid error because of numeric constraint\n",
    "        # TODO: this might cause problems if inference is done on a different device\n",
    "\n",
    "    def sample_opponent(self):\n",
    "        \"\"\"Always return the same model for visualization\"\"\"\n",
    "        return self.model\n",
    "\n",
    "\n",
    "def visualize_self_play(\n",
    "    model_path: str = f\"{DATA_DIR}/self_play_models/final_model\", num_games: int = 1\n",
    "):\n",
    "    # Load the model and create the pool\n",
    "    model_pool = VisualizationModelPool(model_path)\n",
    "\n",
    "    # Create and wrap the environment\n",
    "    env = integrate_with_env(FlexibleRoleDraftEnv)(role_rates_path=role_rates_path)\n",
    "    env = SelfPlayWithPoolWrapper(\n",
    "        env, model_pool, agent_side=\"blue\"\n",
    "    )  # Force blue side for consistency\n",
    "    env = ActionMasker(env, action_mask_fn)\n",
    "\n",
    "    for game in range(num_games):\n",
    "        print(f\"\\nGame {game + 1}:\")\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        while not done and not truncated:\n",
    "            # Get the action mask\n",
    "            action_masks = get_action_masks(env)\n",
    "            # Use the action_masks when predicting the action\n",
    "            action, _states = model_pool.model.predict(\n",
    "                obs, action_masks=action_masks, deterministic=True\n",
    "            )\n",
    "\n",
    "            # Step the environment\n",
    "            obs, reward, done, truncated, info = env.step(int(action))\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode reward (blue side winrate):\", reward)\n",
    "                # Get the final render\n",
    "                image_data = env.render()\n",
    "                if image_data is not None:\n",
    "                    display(IPImage(data=image_data))\n",
    "                else:\n",
    "                    print(\"Draft is not complete or visualization is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View multiple games\n",
    "visualize_self_play(num_games=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image as IPImage, clear_output\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from utils import DATA_DIR\n",
    "from utils.rl.env import FixedRoleDraftEnv, action_mask_fn\n",
    "from utils.rl.self_play import ModelPool\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Optional, Set\n",
    "from difflib import get_close_matches\n",
    "from utils.rl.champions import Champion\n",
    "from gymnasium import Wrapper\n",
    "import sys\n",
    "import time\n",
    "import torch as th\n",
    "\n",
    "\n",
    "class HumanPlayWrapper(Wrapper):\n",
    "    \"\"\"Wrapper that allows human-model interaction from either side\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        human_side: int,  # 0 for blue, 1 for red\n",
    "        model,\n",
    "        role_names: Optional[List[str]] = None,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.human_side = human_side\n",
    "        self.model = model\n",
    "        self.role_names = role_names or [\"TOP\", \"JUNGLE\", \"MID\", \"BOT\", \"UTILITY\"]\n",
    "\n",
    "        # Create lookup dictionaries for champion names\n",
    "        self.id_to_name = {champion.id: champion.display_name for champion in Champion}\n",
    "        self.name_to_id = {\n",
    "            champion.display_name.lower(): champion.id for champion in Champion\n",
    "        }\n",
    "        # Add common abbreviations\n",
    "        self.name_to_id.update(\n",
    "            {\n",
    "                \"tf\": self.name_to_id[\"twisted fate\"],\n",
    "                \"mf\": self.name_to_id[\"miss fortune\"],\n",
    "                \"asol\": self.name_to_id[\"aurelion sol\"],\n",
    "                \"j4\": self.name_to_id[\"jarvan iv\"],\n",
    "                \"tk\": self.name_to_id[\"tahm kench\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Track bans\n",
    "        self.blue_bans: List[int] = []\n",
    "        self.red_bans: List[int] = []\n",
    "\n",
    "        # Track draft history\n",
    "        self.draft_history: List[dict] = []\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset the environment and display initial state\"\"\"\n",
    "        self.blue_bans = []\n",
    "        self.red_bans = []\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        self.display_state()\n",
    "        return obs\n",
    "\n",
    "    def _get_model_suggestions(\n",
    "        self, obs: dict, action_mask: np.ndarray, n_suggestions: int = 5\n",
    "    ) -> List[tuple]:\n",
    "        \"\"\"Get top n suggested actions from the model with their probabilities\"\"\"\n",
    "        # Get action probabilities from model's policy\n",
    "        obs_tensor = {\n",
    "            k: (\n",
    "                th.from_numpy(v).unsqueeze(0).to(self.model.device)\n",
    "                if len(v.shape) == 1\n",
    "                else th.from_numpy(v).unsqueeze(0).to(self.model.device)\n",
    "            )\n",
    "            for k, v in obs.items()\n",
    "        }\n",
    "        with th.no_grad():\n",
    "            action_probs = (\n",
    "                self.model.policy.get_distribution(obs_tensor)\n",
    "                .distribution.probs[0]\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "        # Mask invalid actions\n",
    "        action_probs = action_probs * action_mask\n",
    "\n",
    "        # Get top n valid actions\n",
    "        top_indices = np.argsort(action_probs)[-n_suggestions:][::-1]\n",
    "\n",
    "        return [(idx, action_probs[idx]) for idx in top_indices]\n",
    "\n",
    "    def display_state(self):\n",
    "        \"\"\"Display current draft state in a compact tabular format\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        action_info = self.env.get_current_draft_step()\n",
    "\n",
    "        # Color constants\n",
    "        BLUE = \"\\033[94m\"\n",
    "        RED = \"\\033[91m\"\n",
    "        BAN = \"\\033[93m\"  # Yellow\n",
    "        PICK = \"\\033[92m\"  # Green\n",
    "        RESET = \"\\033[0m\"\n",
    "\n",
    "        current_phase = (\n",
    "            f\"{BAN}BAN{RESET}\" if action_info[\"phase\"] == 0 else f\"{PICK}PICK{RESET}\"\n",
    "        )\n",
    "        current_team = (\n",
    "            f\"{RED}RED{RESET}\" if action_info[\"team\"] == 1 else f\"{BLUE}BLUE{RESET}\"\n",
    "        )\n",
    "\n",
    "        # Get current suggestions\n",
    "        obs = self._get_obs()\n",
    "        action_mask = self.env.get_action_mask()\n",
    "        suggestions = self._get_model_suggestions(obs, action_mask)\n",
    "        suggested_champs = [\n",
    "            self.id_to_name.get(int(champ_id), f\"Unknown({champ_id})\")\n",
    "            for champ_id, _ in suggestions\n",
    "        ]\n",
    "\n",
    "        # Print current state first\n",
    "        print(f\"\\nCurrent Move: {current_team} {current_phase}\")\n",
    "        print(f\"Model suggests: {', '.join(suggested_champs)}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        # Print bans table\n",
    "        print(\"\\nBans:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{'Phase':<10}{BLUE}BLUE{RESET:<20}{RED}RED{RESET:<20}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        max_bans = max(len(self.blue_bans), len(self.red_bans))\n",
    "        for i in range(max_bans):\n",
    "            blue_ban = (\n",
    "                self.id_to_name[self.blue_bans[i]] if i < len(self.blue_bans) else \"---\"\n",
    "            )\n",
    "            red_ban = (\n",
    "                self.id_to_name[self.red_bans[i]] if i < len(self.red_bans) else \"---\"\n",
    "            )\n",
    "            print(f\"{BAN}Ban {i+1}{RESET:<6}{blue_ban:<20}{red_ban:<20}\")\n",
    "\n",
    "        # Print team compositions\n",
    "        print(\"\\nTeam Compositions:\")\n",
    "        print(\"-\" * 100)\n",
    "        roles = [\"TOP\", \"JNG\", \"MID\", \"BOT\", \"SUP\"]\n",
    "        print(f\"{'Role':<10}{BLUE}BLUE{RESET:<20}{RED}RED{RESET:<20}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for i, role in enumerate(roles):\n",
    "            blue_champ = \"---\"\n",
    "            if self.env.blue_roles_picked[i]:\n",
    "                champ_id = np.argmax(self.env.blue_picks[i])\n",
    "                blue_champ = self.id_to_name.get(champ_id, \"???\")\n",
    "\n",
    "            red_champ = \"---\"\n",
    "            if self.env.red_roles_picked[i]:\n",
    "                champ_id = np.argmax(self.env.red_picks[i])\n",
    "                red_champ = self.id_to_name.get(champ_id, \"???\")\n",
    "\n",
    "            print(f\"{role:<10}{blue_champ:<20}{red_champ:<20}\")\n",
    "\n",
    "        # Print draft history with suggestions\n",
    "        print(\"\\nDraft History:\")\n",
    "        print(\"-\" * 100)\n",
    "        for i, move in enumerate(self.draft_history):\n",
    "            phase = f\"{BAN}BAN{RESET}\" if move[\"phase\"] == 0 else f\"{PICK}PICK{RESET}\"\n",
    "            team = f\"{RED}RED{RESET}\" if move[\"team\"] == 1 else f\"{BLUE}BLUE{RESET}\"\n",
    "            chosen = self.id_to_name[move[\"chosen_action\"]]\n",
    "            suggestions = [\n",
    "                self.id_to_name[champ_id] for champ_id, _ in move[\"suggestions\"]\n",
    "            ]\n",
    "\n",
    "            print(f\"Step {i+1}: {team} {phase}\")\n",
    "            print(f\"Chosen: {chosen}\")\n",
    "            print(f\"Top suggestions were: {', '.join(suggestions)}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    def step(self, base_action):\n",
    "        \"\"\"Handle both model and human turns\"\"\"\n",
    "        action_info = self.env.get_current_draft_step()\n",
    "        phase = action_info[\"phase\"]\n",
    "        current_team = action_info[\"team\"]\n",
    "\n",
    "        # Get current state info before action\n",
    "        obs = self._get_obs()\n",
    "        action_mask = self.env.get_action_mask()\n",
    "        suggestions = self._get_model_suggestions(obs, action_mask)\n",
    "\n",
    "        # Get action based on current turn\n",
    "        if current_team == self.human_side:\n",
    "            action = self._get_human_action(phase)\n",
    "        else:\n",
    "            action = self._get_model_action(phase)\n",
    "\n",
    "        # Store draft history\n",
    "        chosen_action = action[0] if isinstance(action, (list, np.ndarray)) else action\n",
    "\n",
    "        self.draft_history.append(\n",
    "            {\n",
    "                \"phase\": phase,\n",
    "                \"team\": current_team,\n",
    "                \"chosen_action\": chosen_action,\n",
    "                \"suggestions\": suggestions,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Track bans\n",
    "        if phase == 0:\n",
    "            if current_team == 0:  # Blue side\n",
    "                self.blue_bans.append(chosen_action)\n",
    "            else:  # Red side\n",
    "                self.red_bans.append(chosen_action)\n",
    "\n",
    "        result = self.env.step(chosen_action)\n",
    "        return result\n",
    "\n",
    "    def _get_human_action(self, phase: int) -> int:\n",
    "        \"\"\"Get action from human input\"\"\"\n",
    "        self.display_state()\n",
    "        sys.stdout.flush()  # ensure state is displayed before input\n",
    "        action_mask = self.env.get_action_mask()\n",
    "        valid_actions = set(np.where(action_mask == 1)[0])\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                prompt = (\n",
    "                    \"\\nEnter champion name to ban: \"\n",
    "                    if phase == 0\n",
    "                    else \"\\nEnter champion name to pick: \"\n",
    "                )\n",
    "                search = input(prompt)\n",
    "                chosen_id = self._process_human_input(search, valid_actions)\n",
    "\n",
    "                if chosen_id is not None:\n",
    "                    return chosen_id\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                print(\"Please try again.\")\n",
    "\n",
    "    def _get_model_action(self, phase: int) -> np.ndarray:\n",
    "        \"\"\"Get action from model prediction\"\"\"\n",
    "        obs = self._get_obs()\n",
    "        action_mask = self.env.get_action_mask()\n",
    "\n",
    "        action, _states = self.model.predict(\n",
    "            obs,\n",
    "            action_masks=np.array([action_mask]),\n",
    "            deterministic=True,\n",
    "        )\n",
    "        return action\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Get observation in the format expected by the model\"\"\"\n",
    "        return {\n",
    "            \"available_champions\": self.env.available_champions.copy(),\n",
    "            \"blue_picks\": self.env.blue_picks.copy(),\n",
    "            \"red_picks\": self.env.red_picks.copy(),\n",
    "            \"blue_ordered_picks\": self.env.blue_ordered_picks.copy(),\n",
    "            \"red_ordered_picks\": self.env.red_ordered_picks.copy(),\n",
    "            \"blue_roles_picked\": self.env.blue_roles_picked.copy(),\n",
    "            \"red_roles_picked\": self.env.red_roles_picked.copy(),\n",
    "            \"phase\": np.array(\n",
    "                [self.env.get_current_draft_step()[\"phase\"]], dtype=np.int8\n",
    "            ),\n",
    "            \"turn\": np.array(\n",
    "                [self.env.get_current_draft_step()[\"team\"]], dtype=np.int8\n",
    "            ),\n",
    "            \"action_mask\": self.env.get_action_mask(),\n",
    "        }\n",
    "\n",
    "    def _process_human_input(\n",
    "        self, search: str, valid_actions: Set[int]\n",
    "    ) -> Optional[int]:\n",
    "        \"\"\"Process human input and return chosen champion ID if valid\"\"\"\n",
    "        matches = self.find_champion(search)\n",
    "\n",
    "        if not matches:\n",
    "            print(\"No champions found matching that name. Please try again.\")\n",
    "            return None\n",
    "\n",
    "        if len(matches) > 1:\n",
    "            valid_matches = [m for m in matches if m[0] in valid_actions]\n",
    "\n",
    "            if not valid_matches:\n",
    "                print(\"None of the matches are available. Please try again.\")\n",
    "                return None\n",
    "\n",
    "            print(\"\\nDid you mean:\")\n",
    "            for i, (champ_id, champ_name) in enumerate(valid_matches, 1):\n",
    "                print(f\"{i}. {champ_name}\")\n",
    "\n",
    "            choice = input(\"Enter number (or press Enter to search again): \")\n",
    "            if not choice:\n",
    "                return None\n",
    "\n",
    "            try:\n",
    "                chosen_id = valid_matches[int(choice) - 1][0]\n",
    "            except (ValueError, IndexError):\n",
    "                print(\"Invalid choice. Please try again.\")\n",
    "                return None\n",
    "        else:\n",
    "            chosen_id = matches[0][0]\n",
    "\n",
    "        if chosen_id in valid_actions:\n",
    "            return chosen_id\n",
    "\n",
    "        print(\n",
    "            f\"{self.id_to_name[chosen_id]} is not available. Please choose from the valid champions listed above.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    def find_champion(self, search_term: str) -> List[tuple]:\n",
    "        \"\"\"Find champions matching the search term\"\"\"\n",
    "        search_term = search_term.lower().strip()\n",
    "\n",
    "        # Direct match with name or abbreviation\n",
    "        if search_term in self.name_to_id:\n",
    "            champ_id = self.name_to_id[search_term]\n",
    "            return [(champ_id, self.id_to_name[champ_id])]\n",
    "\n",
    "        # Try to find close matches\n",
    "        all_names = list(self.name_to_id.keys())\n",
    "        matches = get_close_matches(search_term, all_names, n=3, cutoff=0.6)\n",
    "\n",
    "        return [\n",
    "            (self.name_to_id[name], self.id_to_name[self.name_to_id[name]])\n",
    "            for name in matches\n",
    "        ]\n",
    "\n",
    "\n",
    "def play_vs_model(\n",
    "    human_side: str = \"blue\",\n",
    "    model_path: str = f\"{DATA_DIR}/self_play_models/final_model\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Play against the trained model.\n",
    "\n",
    "    Args:\n",
    "        human_side: \"blue\" or \"red\"\n",
    "        model_path: path to the trained model\n",
    "    \"\"\"\n",
    "    # Convert side string to integer\n",
    "    human_side_int = 0 if human_side.lower() == \"blue\" else 1\n",
    "\n",
    "    # Load the model\n",
    "    model = MaskablePPO.load(model_path, device=device)\n",
    "\n",
    "    # Create and wrap the environment\n",
    "    env = integrate_with_env(FixedRoleDraftEnv)()\n",
    "    env = HumanPlayWrapper(env, human_side_int, model)\n",
    "    env = ActionMasker(env, action_mask_fn)\n",
    "\n",
    "    # Reset the environment\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Take step in environment\n",
    "        action = [0]  # Dummy action, will be handled by wrapper\n",
    "        obs, reward, done, truncated, info = env.step(\n",
    "            action\n",
    "        )  # TODO: weird way to go about it, maybe should instead use a function that returns an action?\n",
    "\n",
    "    env.display_state()  # Display final state\n",
    "\n",
    "    print(\"\\nDraft Complete!\")\n",
    "    if human_side_int == 0:\n",
    "        print(\"Final winrate prediction (your side):\", reward)\n",
    "    else:\n",
    "        print(\"Final winrate prediction (your side):\", 1 - reward)\n",
    "\n",
    "    # Get the final render\n",
    "    image_data = env.render()\n",
    "    if image_data is not None:\n",
    "        display(IPImage(data=image_data))\n",
    "    else:\n",
    "        print(\"Visualization not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_vs_model(\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_vs_model(\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
